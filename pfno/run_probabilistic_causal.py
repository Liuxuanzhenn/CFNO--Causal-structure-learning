"""
æ¦‚ç‡å› æœå‘ç°è¿è¡Œè„šæœ¬
ä½¿ç”¨æ¦‚ç‡å‚…é‡Œå¶ç¥ç»ç®—å­è¿›è¡Œå› æœå…³ç³»å‘ç°
å®Œå…¨ä½¿ç”¨pfno_config.pyè¿›è¡Œé…ç½®ç®¡ç†

è¿è¡Œæ–¹å¼:
python run_probabilistic_causal.py --model_type probabilistic --n_samples 20 --confidence_threshold 0.7
"""

import torch
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import argparse
import datetime
from tqdm import tqdm
from torch.utils.data import DataLoader
import seaborn as sns
import time
import hashlib
import random
import yaml
import json
from sklearn.metrics import auc

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

# ä½¿ç”¨ç›¸å¯¹å¯¼å…¥æ›¿ä»£ç»å¯¹å¯¼å…¥
from .probabilistic_causal_pfno_enhanced import EnhancedProbabilisticCausalPFNO

# å¯¼å…¥PFNOä¸“ç”¨é…ç½®
from .config_loader import (
    get_pfno_config, 
    update_config_from_args, 
    print_config_summary,
    save_config_to_file
)
from .utils.config import validate_config

# å¯¼å…¥ç°æœ‰å·¥å…·å‡½æ•° - è¿™äº›æ˜¯é¡¹ç›®çº§åˆ«çš„å·¥å…·ï¼Œéœ€è¦å‘ä¸Šå¯¼å…¥
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ä»¥è®¿é—®é¡¹ç›®çº§å·¥å…·
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from utils import (
        load_data, 
        evaluate_causal_discovery,
        plot_adjacency_matrix,
        set_seed
    )
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥é¡¹ç›®å·¥å…·å‡½æ•°: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# å¯¼å…¥PFNOç»Ÿä¸€å¯è§†åŒ–å¥—ä»¶
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from .utils.visualization import (
        PFNOVisualizationSuite, 
        plot_training_curves, 
        plot_probability_matrix_comparison,
        plot_enhanced_time_series_reconstruction
    )
    PFNO_VIZ_AVAILABLE = True
    print("âœ… PFNOå¯è§†åŒ–å¥—ä»¶å·²åŠ è½½ (ç›¸å¯¹å¯¼å…¥)")
except ImportError as e:
    try:
        # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
        from pfno.utils.visualization import (
            PFNOVisualizationSuite, 
            plot_training_curves, 
            plot_probability_matrix_comparison,
            plot_enhanced_time_series_reconstruction
        )
        PFNO_VIZ_AVAILABLE = True
        print("âœ… PFNOå¯è§†åŒ–å¥—ä»¶å·²åŠ è½½ (ç»å¯¹å¯¼å…¥)")
    except ImportError as e2:
        print(f"âš ï¸ PFNOå¯è§†åŒ–å¥—ä»¶ä¸å¯ç”¨ (ç›¸å¯¹å¯¼å…¥): {e}")
        print(f"âš ï¸ PFNOå¯è§†åŒ–å¥—ä»¶ä¸å¯ç”¨ (ç»å¯¹å¯¼å…¥): {e2}")
        PFNO_VIZ_AVAILABLE = False
        PFNOVisualizationSuite = None
        plot_training_curves = None
        plot_probability_matrix_comparison = None
        plot_enhanced_time_series_reconstruction = None


# æ–°å¢ï¼šç”¨äºç»˜åˆ¶ROCæ›²çº¿çš„å‡½æ•° (é«˜è´¨é‡å‡ºç‰ˆçº§)
def plot_roc_curve(fprs, tprs, roc_auc, model_name, results_dir):
    """ç»˜åˆ¶å¹¶ä¿å­˜ç¬¦åˆæœŸåˆŠå®¡ç¾çš„é«˜è´¨é‡ROCæ›²çº¿å›¾"""
    try:
        # ä½¿ç”¨seabornè¿›è¡Œç¾åŒ–
        sns.set_theme(style="whitegrid")
        plt.figure(figsize=(8, 8))
        
        # ç»˜åˆ¶æˆ‘ä»¬çš„æ¨¡å‹çš„ROCæ›²çº¿ (æ¨¡ä»¿ç¤ºä¾‹ä¸­çš„è“è‰²å®çº¿)
        # ä½¿ç”¨ä¸€ä¸ªä¸“ä¸šä¸”æ¸…æ™°çš„è“è‰²
        plt.plot(fprs, tprs, color='#00529B', lw=2.5, 
                 label=f'{model_name} (AUC = {roc_auc:.3f})')
        
        # ç»˜åˆ¶éšæœºçŒœæµ‹çº¿ (æ¨¡ä»¿ç¤ºä¾‹ä¸­çš„ç°è‰²è™šçº¿)
        plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--', label='Random Guess')
        
        # è®¾ç½®åæ ‡è½´èŒƒå›´å’Œæ ‡ç­¾
        plt.xlim([-0.02, 1.0])
        plt.ylim([0.0, 1.02])
        plt.xlabel('False Positive Rate', fontsize=14)
        plt.ylabel('True Positive Rate', fontsize=14)
        plt.title('ROC Curve Analysis', fontsize=16, pad=12)
        
        # è®¾ç½®åˆ»åº¦å­—ä½“å¤§å°
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        
        # è®¾ç½®å›¾ä¾‹
        plt.legend(loc="lower right", fontsize=12)
        
        # ç§»é™¤é¡¶éƒ¨å’Œå³ä¾§çš„è¾¹æ¡†ä»¥è¾¾åˆ°æ›´ç®€æ´çš„å­¦æœ¯é£æ ¼
        sns.despine()

        # ä¿å­˜å›¾åƒ
        save_path = os.path.join(results_dir, 'roc_curve.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… é«˜è´¨é‡ROCæ›²çº¿å›¾å·²ä¿å­˜: {save_path}")
        return True
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶ROCæ›²çº¿å¤±è´¥: {e}")
        return False


# æ–°å¢ï¼šç”¨äºåºåˆ—åŒ–Numpyå¯¹è±¡çš„JSONç¼–ç å™¨
class NpEncoder(json.JSONEncoder):
    """ è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œç”¨äºå¤„ç†Numpyæ•°æ®ç±»å‹ """
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NpEncoder, self).default(obj)


def setup_seeds_and_reproducibility(config):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯é‡ç°æ€§
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        actual_seed: å®é™…ä½¿ç”¨çš„ç§å­å€¼
    """
    seed_config = config.get('random_seed', {})
    use_fixed_seed = seed_config.get('use_fixed_seed', True)
    base_seed = seed_config.get('fixed_seed', 21)
    
    if use_fixed_seed:
        actual_seed = base_seed
        print(f"ğŸ”§ ä½¿ç”¨å›ºå®šç§å­: {actual_seed}")
    else:
        # ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆéšæœºç§å­
        actual_seed = int(time.time() * 1000) % 100000
        print(f"ğŸ”§ ä½¿ç”¨æ—¶é—´æˆ³ç§å­: {actual_seed}")
    
    # è®¾ç½®æ‰€æœ‰éšæœºæºçš„ç§å­
    torch.manual_seed(actual_seed)
    np.random.seed(actual_seed)
    random.seed(actual_seed)
    
    # è®¾ç½®CUDAç§å­ä»¥ç¡®ä¿GPUè®¡ç®—çš„å¯é‡ç°æ€§
    if torch.cuda.is_available():
        torch.cuda.manual_seed(actual_seed)
        torch.cuda.manual_seed_all(actual_seed)
        # è®¾ç½®CUDAä¸ºç¡®å®šæ€§æ¨¡å¼
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    # ä¿å­˜ç§å­ä¿¡æ¯
    if seed_config.get('save_seed', True):
        seed_file = seed_config.get('seed_file', 'last_seed.txt')
        with open(seed_file, 'w') as f:
            f.write(f"seed: {actual_seed}\n")
            f.write(f"use_fixed_seed: {use_fixed_seed}\n")
    
    return actual_seed


def setup_experiment():
    """è®¾ç½®å®éªŒå‚æ•°å’Œç¯å¢ƒ"""
    # åŠ è½½pfnoé…ç½®çš„é»˜è®¤å€¼
    pfno_cfg = get_pfno_config()
    
    parser = argparse.ArgumentParser(description='æ¦‚ç‡å› æœå‘ç°å®éªŒ')
    
    # æ–°å¢ï¼šç”¨äºä¼˜åŒ–çš„å‚æ•°
    parser.add_argument('--config', type=str, default=None, help='ä»æŒ‡å®šçš„YAMLæ–‡ä»¶åŠ è½½é…ç½®')
    parser.add_argument('--output_json', type=str, default=None, help='å°†æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡ä¿å­˜ä¸ºJSONæ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--no-viz', action='store_true', help='ç¦ç”¨æ‰€æœ‰å¯è§†åŒ–è¾“å‡ºï¼ŒåŠ é€Ÿä¼˜åŒ–è¿‡ç¨‹')

    # ç§»é™¤model_typeå‚æ•°ï¼Œåªä¿ç•™PFNO
    # ç§»é™¤é¢„è®¾é…ç½®åŠŸèƒ½
    
    # ä½¿ç”¨pfno_configä¸­çš„é»˜è®¤å€¼ï¼Œåªæœ‰åœ¨å‘½ä»¤è¡Œæ˜ç¡®æŒ‡å®šæ—¶æ‰è¦†ç›–
    parser.add_argument('--n_samples', type=int, 
                       default=pfno_cfg['probabilistic']['n_samples'], 
                       help=f'æ¦‚ç‡é‡‡æ ·æ•°é‡ (é»˜è®¤: {pfno_cfg["probabilistic"]["n_samples"]})')
    parser.add_argument('--confidence_threshold', type=float, 
                       default=pfno_cfg['probabilistic']['confidence_threshold'], 
                       help=f'ç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤: {pfno_cfg["probabilistic"]["confidence_threshold"]})')
    parser.add_argument('--uncertainty_threshold', type=float, 
                       default=pfno_cfg['probabilistic']['uncertainty_threshold'], 
                       help=f'ä¸ç¡®å®šæ€§é˜ˆå€¼ (é»˜è®¤: {pfno_cfg["probabilistic"]["uncertainty_threshold"]})')
    parser.add_argument('--num_epochs', type=int, 
                       default=pfno_cfg['training']['num_epochs'], 
                       help=f'è®­ç»ƒè½®æ•° (é»˜è®¤: {pfno_cfg["training"]["num_epochs"]})')
    parser.add_argument('--learning_rate', type=float, 
                       default=pfno_cfg['training']['learning_rate'], 
                       help=f'å­¦ä¹ ç‡ (é»˜è®¤: {pfno_cfg["training"]["learning_rate"]})')
    
    args = parser.parse_args()
    
    # è·å–é»˜è®¤é…ç½®æˆ–ä»æ–‡ä»¶åŠ è½½
    if args.config:
        print(f"ğŸ§¬ ä»æ–‡ä»¶åŠ è½½é…ç½®: {args.config}")
        if not os.path.exists(args.config):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {args.config}")
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        print("ğŸ§¬ ä½¿ç”¨é»˜è®¤é…ç½® (from pfno_config.py)")
        config = get_pfno_config()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®
    config = update_config_from_args(config, args)
    
    # éªŒè¯é…ç½®
    if not validate_config(config):
        raise ValueError("é…ç½®éªŒè¯å¤±è´¥")
    
    # è®¾ç½®éšæœºç§å­
    actual_seed = setup_seeds_and_reproducibility(config)
    config['runtime'] = config.get('runtime', {})
    config['runtime']['actual_seed'] = actual_seed
    
    # è®¾ç½®ç»“æœç›®å½•
    # å¦‚æœoutput_jsonè¢«æŒ‡å®š(æ„å‘³ç€åœ¨ä¼˜åŒ–å¾ªç¯ä¸­)ï¼Œåˆ™ç®€åŒ–ç›®å½•ç»“æ„
    if args.output_json:
        # åœ¨ä¼˜åŒ–æ¨¡å¼ä¸‹ï¼Œç»“æœå¯ä»¥ä¿å­˜åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç›®å½•ï¼Œé¿å…åˆ›å»ºè¿‡å¤šæ–‡ä»¶å¤¹
        results_dir = os.path.join("results", "hpo_runs", f"trial_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S_%f')}")
    else:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = f"results/probabilistic_causal_pfno_{timestamp}"

    os.makedirs(results_dir, exist_ok=True)
    
    print(f"=== æ¦‚ç‡å› æœå‘ç°å®éªŒ ===")
    print(f"æ•°æ®æ–‡ä»¶: {config['data']['file_path']}")
    print(f"çœŸå®å› æœçŸ©é˜µ: {config['data']['ground_truth_file']}")
    print(f"æ¨¡å‹ç±»å‹: PFNO")
    print(f"ç»“æœç›®å½•: {results_dir}")
    print(f"ä½¿ç”¨éšæœºç§å­: {actual_seed}")
    print("-" * 50)
    
    # æ‰“å°é…ç½®æ‘˜è¦
    print_config_summary(config)
    
    # ä¿å­˜é…ç½®åˆ°ç»“æœç›®å½•
    save_config_to_file(config, os.path.join(results_dir, 'experiment_config.json'))
    
    return args, config, results_dir, actual_seed


def load_experimental_data(config):
    """åŠ è½½å®éªŒæ•°æ® - æ”¹è¿›çš„æ•°æ®åŠ è½½ç³»ç»Ÿ"""
    try:
        # å°è¯•ä½¿ç”¨å¢å¼ºé¢„å¤„ç†ç³»ç»Ÿ
        from .preprocessing.data_loader import load_experimental_data_enhanced
        
        print(f"ğŸš€ ä½¿ç”¨å¢å¼ºé¢„å¤„ç†ç³»ç»ŸåŠ è½½æ•°æ®...")
        
        # æ˜¾ç¤ºé¢„å¤„ç†é…ç½®ä¿¡æ¯
        preprocessing_method = config['data']['preprocessing']['method']
        print(f"ğŸ“‹ é¢„å¤„ç†æ–¹æ³•: {preprocessing_method}")
        
        # ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒæ–°çš„é¢„å¤„ç†ç³»ç»Ÿï¼‰
        train_dataloader, val_dataloader, seq_length, num_nodes, ground_truth, raw_data = load_experimental_data_enhanced(config)
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: åºåˆ—é•¿åº¦={seq_length}, èŠ‚ç‚¹æ•°={num_nodes}")
        
        return train_dataloader, val_dataloader, seq_length, num_nodes, ground_truth, raw_data
        
    except ImportError as e:
        print(f"âš ï¸ å¢å¼ºé¢„å¤„ç†ç³»ç»Ÿä¸å¯ç”¨: {e}")
        print(f"ğŸ”„ å›é€€åˆ°æ ‡å‡†æ•°æ®åŠ è½½æ–¹æ³•...")
        
        # å›é€€åˆ°æ ‡å‡†æ•°æ®åŠ è½½
        from utils import load_data
        
        train_dataloader, val_dataloader = load_data(
            file_path=config['data']['file_path'],
            train_ratio=config['data']['train_ratio'],
            batch_size=config['training']['batch_size'],
            sequence_length=config['data']['sequence_length'],
            max_time_steps=config['data']['max_time_steps'],
            normalize=True,
            augment=config['data']['augment']
        )
        
        # æ¨æ–­æ•°æ®ç»´åº¦
        raw_data = None
        for batch_x, batch_y in train_dataloader:
            total_features = batch_x.shape[1]
            seq_length = config['data']['sequence_length']
            num_nodes = total_features // seq_length
            
            # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºå¯è§†åŒ–
            raw_data = batch_x[0].detach().cpu().numpy() if isinstance(batch_x[0], torch.Tensor) else batch_x[0]
            break
        
        # å°è¯•ç›´æ¥ä»æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®ç”¨äºæ—¶é—´åºåˆ—å¯è§†åŒ–
        if raw_data is None:
            try:
                import pandas as pd
                original_df = pd.read_csv(config['data']['file_path'])
                if len(original_df.columns) >= num_nodes:
                    # å–å‰num_nodesåˆ—å’Œå‰seq_lengthè¡Œ
                    raw_data = original_df.iloc[:seq_length, :num_nodes].values
                    print(f"   ğŸ“Š ä»æ–‡ä»¶ç›´æ¥åŠ è½½åŸå§‹æ•°æ®ç”¨äºå¯è§†åŒ–: {raw_data.shape}")
            except Exception as e:
                print(f"   âš ï¸ æ— æ³•ä»æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®: {e}")
        
        # å°è¯•åŠ è½½çœŸå®é‚»æ¥çŸ©é˜µ
        try:
            ground_truth = pd.read_csv(config['data']['ground_truth_file'], index_col=0).values
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½çœŸå®é‚»æ¥çŸ©é˜µ: {e}")
            ground_truth = None
        
        print(f"âœ… æ ‡å‡†æ•°æ®åŠ è½½å®Œæˆ: åºåˆ—é•¿åº¦={seq_length}, èŠ‚ç‚¹æ•°={num_nodes}")
        print(f"   ğŸ“ˆ åŸå§‹æ•°æ®å¯ç”¨äºå¯è§†åŒ–: {'æ˜¯' if raw_data is not None else 'å¦'}")
        
        return train_dataloader, val_dataloader, seq_length, num_nodes, ground_truth, raw_data


def create_probabilistic_model(num_nodes, seq_length, config):
    """ä»é…ç½®åˆ›å»ºæ¦‚ç‡å› æœæ¨¡å‹"""
    model_config = config['architecture']
    
    model = EnhancedProbabilisticCausalPFNO(
        n_modes=model_config['n_modes'],
        hidden_channels=model_config['hidden_channels'],
        lifting_channels=model_config['lifting_channels'],
        projection_channels=model_config['projection_channels'],
        num_nodes=num_nodes,
        sequence_length=seq_length,
        in_channels=model_config.get('in_channels', 1),
        n_layers=model_config['n_layers'],
        dropout=model_config['dropout'],
        n_attn_heads=model_config.get('n_attn_heads', 4),
        fno_output_layer_indices=model_config.get('fno_output_layer_indices')
    )
    
    print("é‡æ„ç‰ˆæ³¨æ„åŠ›å› æœPFNOæ¨¡å‹å·²åˆ›å»º:")
    print(f"   - èŠ‚ç‚¹æ•°: {num_nodes}, åºåˆ—é•¿åº¦: {seq_length}")
    print(f"   - Lifting/Hidden/Projection é€šé“: {model_config['lifting_channels']}/{model_config['hidden_channels']}/{model_config['projection_channels']}")
    print(f"   - FNOæ¨¡å¼: {model_config['n_modes']}, FNOå±‚æ•°: {model_config['n_layers']}")
    print(f"   - æ³¨æ„åŠ›å¤´æ•°: {model_config.get('n_attn_heads', 4)}")
    
    return model


def train_probabilistic_model(model, train_dataloader, val_dataloader, config, results_dir):
    """è®­ç»ƒæ¦‚ç‡æ¨¡å‹ - æ”¹è¿›çš„è®­ç»ƒè¿‡ç¨‹ï¼Œè§£å†³å±€éƒ¨æœ€ä¼˜é—®é¢˜"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # ğŸ”§ æ”¹è¿›çš„ä¼˜åŒ–å™¨é…ç½® - è§£å†³å±€éƒ¨æœ€ä¼˜å’Œç±»å‹å†²çªé—®é¢˜
    real_params = []
    complex_params = []
    
    for name, param in model.named_parameters():
        if param.is_complex():
            complex_params.append(param)
        else:
            real_params.append(param)
    
    # ä¸ºé‚»æ¥çŸ©é˜µå‚æ•°è®¾ç½®æ›´é«˜çš„å­¦ä¹ ç‡ï¼Œå¸®åŠ©è·³å‡ºå±€éƒ¨æœ€ä¼˜
    base_lr = config['training']['learning_rate']
    param_groups = []
    
    # å®æ•°å‚æ•°ç»„ï¼šå¯ä»¥ä½¿ç”¨amsgrad
    if real_params:
        param_groups.append({
            'params': real_params,
            'lr': base_lr,
            'weight_decay': config['training']['weight_decay'],
            'amsgrad': False, # è¯Šæ–­æ­¥éª¤ï¼šæš‚æ—¶ç¦ç”¨ä»¥é¿å…ä¸å¤æ•°å‚æ•°çš„æ·±å±‚å†²çª
            'name': 'real_params'
        })
    
    # å¤æ•°å‚æ•°ç»„ï¼šå¿…é¡»ç¦ç”¨amsgrad
    if complex_params:
        param_groups.append({
            'params': complex_params,
            'lr': base_lr,
            'weight_decay': 0, # é€šå¸¸ä¸å¯¹å¤æ•°å‚æ•°åº”ç”¨æƒé‡è¡°å‡
            'amsgrad': False, # ç¦ç”¨amsgradä»¥é¿å…ç±»å‹å†²çª
            'name': 'complex_params'
        })
    
    # å¦‚æœæ²¡æœ‰åˆ†ç»„ï¼Œä½¿ç”¨æ‰€æœ‰å‚æ•°ï¼ˆè¿™ç§æƒ…å†µç°åœ¨ä¸å¤ªå¯èƒ½å‘ç”Ÿï¼‰
    if not param_groups:
        param_groups = [{'params': model.parameters(), 'lr': base_lr}]
    
    # ä½¿ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–å™¨é…ç½®
    optimizer = torch.optim.AdamW(
        param_groups,
        betas=(0.9, 0.999),
        eps=1e-8
        # amsgrad åœ¨å‚æ•°ç»„çº§åˆ«è®¾ç½®
    )
    
    # ğŸ”§ æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«é‡å¯ï¼Œå¸®åŠ©è·³å‡ºå±€éƒ¨æœ€ä¼˜
    scheduler_config = config['training'].get('scheduler', {})
    scheduler_name = scheduler_config.get('name', 'CosineAnnealingWarmRestarts') # é»˜è®¤ä¸ºè¿™ä¸ª

    if scheduler_name == 'CosineAnnealingWarmRestarts':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=scheduler_config.get('T_0', 20),
            T_mult=scheduler_config.get('T_mult', 2),
            eta_min=scheduler_config.get('eta_min', base_lr * 0.01)
        )
        print("ğŸ”§ ä½¿ç”¨ CosineAnnealingWarmRestarts å­¦ä¹ ç‡è°ƒåº¦å™¨")
    else:
        # å¯ä»¥æ·»åŠ å…¶ä»–è°ƒåº¦å™¨æˆ–å›é€€åˆ°é»˜è®¤
        print("âš ï¸ æœªçŸ¥çš„è°ƒåº¦å™¨åç§°ï¼Œå›é€€åˆ° ReduceLROnPlateau")
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

    
    print(f"\n{'='*60}")
    print(f"              å¼€å§‹æ¨¡å‹è®­ç»ƒ (æ”¹è¿›ç‰ˆ)")
    print(f"{'='*60}")
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ”§ ä¼˜åŒ–å™¨: {config['training']['optimizer']}")
    print(f"ğŸ”§ å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    print(f"ğŸ”§ è®­ç»ƒè½®æ•°: {config['training']['num_epochs']}")
    print(f"ğŸ”§ æ‰¹é‡å¤§å°: {config['training']['batch_size']}")
    print(f"ğŸ”§ æ—©åœè€å¿ƒå€¼: {config['training']['patience']}")
    print(f"ğŸ”§ DAGçº¦æŸ: {config['causal']['use_dag_constraint']}")
    print(f"ğŸ”§ æ¢¯åº¦è£å‰ª: {config['training']['gradient_clipping']}")
    print(f"{'='*60}\n")
    
    # è®­ç»ƒå¾ªç¯çŠ¶æ€
    train_losses = []
    val_losses = []
    loss_components = {
        'reconstruction': [],
        'sparsity': [],
        'dag_constraint': [],
        'uncertainty': [],
        'scale_regularization': [],
        'l1_regularization': [],        # ğŸ”§ æ¦‚ç‡å­¦ä¹ å™¨L1æ­£åˆ™åŒ–è®°å½•
        'l2_regularization': []         # ğŸ”§ æ¦‚ç‡å­¦ä¹ å™¨L2æ­£åˆ™åŒ–è®°å½•
    }
    
    best_val_loss = float('inf')
    patience_counter = 0
    
    # è·å–æŸå¤±æƒé‡
    loss_weights = config['training']['loss_weights']
    # ä¿®æ­£ï¼šå°†ç‹¬ç«‹çš„DAGæƒé‡æ·»åŠ åˆ°æŸå¤±æƒé‡å­—å…¸ä¸­ï¼Œç¡®ä¿é…ç½®ç”Ÿæ•ˆ
    if 'causal' in config and 'causal_reg_weight' in config['causal']:
        loss_weights['dag_constraint'] = config['causal']['causal_reg_weight']
        print(f"ğŸ”§ å·²ä»é…ç½®åŠ è½½DAGçº¦æŸæƒé‡: {loss_weights['dag_constraint']:.4f}")
    
    # è·å–åŠ¨æ€DAGæƒé‡é…ç½®
    dynamic_dag_config = loss_weights.get('dynamic_dag_weight', {})
    dynamic_dag_enabled = dynamic_dag_config.get('enabled', False)
    if dynamic_dag_enabled:
        print(f"ğŸ“ˆ åŠ¨æ€DAGæƒé‡å·²å¯ç”¨: ä» epoch {dynamic_dag_config.get('start_epoch', 20)} "
              f"åˆ° epoch {dynamic_dag_config.get('end_epoch', 45)}, "
              f"æƒé‡ä» {loss_weights.get('dag_constraint', 0.001):.4f} "
              f"å¢åŠ åˆ° {dynamic_dag_config.get('end_weight', 0.1):.4f}")

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['training']['num_epochs']):
        # --- åŠ¨æ€DAGæƒé‡è®¡ç®— ---
        current_dag_weight = loss_weights.get('dag_constraint', 0.001)
        if dynamic_dag_enabled:
            start_epoch = dynamic_dag_config.get('start_epoch', 20)
            end_epoch = dynamic_dag_config.get('end_epoch', 45)
            start_weight = loss_weights.get('dag_constraint', 0.001) # åˆå§‹æƒé‡
            end_weight = dynamic_dag_config.get('end_weight', 0.1)

            if epoch >= start_epoch:
                if epoch >= end_epoch:
                    current_dag_weight = end_weight
                else:
                    # çº¿æ€§æ’å€¼
                    progress = (epoch - start_epoch) / (end_epoch - start_epoch)
                    current_dag_weight = start_weight + progress * (end_weight - start_weight)
        
        # å°†è®¡ç®—å‡ºçš„å½“å‰DAGæƒé‡æ›´æ–°åˆ°loss_weightså­—å…¸ä¸­ï¼Œä»¥ä¾¿ä¼ é€’ç»™get_loss
        dynamic_loss_weights = loss_weights.copy()
        dynamic_loss_weights['dag_constraint'] = current_dag_weight
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()  # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
        epoch_train_loss = 0
        epoch_recon_loss = 0 # æ–°å¢ï¼šå•ç‹¬è·Ÿè¸ªé‡æ„æŸå¤±
        epoch_dag_loss = 0  # å•ç‹¬è·Ÿè¸ªDAGæŸå¤±
        
        with tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{config['training']['num_epochs']} - Training", 
                 leave=False) as train_pbar:
            for batch_idx, (batch_x, batch_y) in enumerate(train_pbar):
                try:
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    
                    # å‰å‘ä¼ æ’­
                    result = model(batch_x)
                    prediction_mean = result['prediction_mean']
                    prediction_log_var = result['prediction_log_var']
                    causal_matrix = result['causal_matrix']
                    causal_logits = result['causal_logits'] # è·å–logits
                    
                    # è®¡ç®—æŸå¤± (ä½¿ç”¨åŒ…å«åŠ¨æ€æƒé‡çš„å­—å…¸)
                    total_loss, loss_dict = model.get_loss(
                        prediction_mean, 
                        prediction_log_var,
                        batch_y, 
                        causal_matrix, 
                        causal_logits, # ä¼ é€’logits
                        loss_weights=dynamic_loss_weights
                    )
                    
                    # ç´¯ç§¯æŸå¤±ç»„ä»¶ç”¨äºåˆ†æ - ä½¿ç”¨detach()é¿å…è®¡ç®—å›¾é—®é¢˜
                    for key, value in loss_dict.items():
                        if key not in loss_components:
                            loss_components[key] = []
                        # ç¡®ä¿å°†tensorè½¬æ¢ä¸ºfloatå­˜å‚¨ç”¨äºè®°å½•ï¼Œå¹¶detachåˆ‡æ–­æ¢¯åº¦
                        if isinstance(value, torch.Tensor):
                            loss_components[key].append(value.detach().item())  # æ·»åŠ detach()
                        else:
                            loss_components[key].append(float(value))
                    
                    # åå‘ä¼ æ’­ - ğŸ”§ ç¡®ä¿åªè°ƒç”¨ä¸€æ¬¡backwardï¼Œæ·»åŠ å¼‚å¸¸å¤„ç†
                    optimizer.zero_grad()
                    try:
                        total_loss.backward()  # total_lossç°åœ¨å·²ç»åŒ…å«YAMLæƒé‡
                    except RuntimeError as e:
                        if "second time" in str(e) or "already been freed" in str(e):
                            print(f"âš ï¸ è·³è¿‡æ‰¹æ¬¡ {batch_idx}: è®¡ç®—å›¾é‡å¤ä½¿ç”¨é”™è¯¯")
                            # æ¸…ç†æ¢¯åº¦å¹¶è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
                            optimizer.zero_grad()
                            continue
                        else:
                            # é‡æ–°æŠ›å‡ºå…¶ä»–ç±»å‹çš„é”™è¯¯
                            raise e
                    
                    # æ¢¯åº¦è£å‰ª - è¯Šæ–­æ­¥éª¤ï¼šæš‚æ—¶æ³¨é‡Šæ‰ï¼Œä»¥æ’æŸ¥ä¸å¤æ•°å‚æ•°çš„å†²çª
                    # if 'gradient_clipping' in config['training']:
                    #     torch.nn.utils.clip_grad_norm_(model.parameters(), config['training']['gradient_clipping'])
                    
                    optimizer.step()
                    
                    # ğŸ”§ total_losså·²ç»æ˜¯YAMLæƒé‡è®¡ç®—çš„ç»“æœï¼Œç›´æ¥ä½¿ç”¨å¹¶detach
                    epoch_train_loss += total_loss.detach().item()  # æ·»åŠ detach()
                    epoch_recon_loss += loss_dict['recon_loss'].detach().item() if isinstance(loss_dict['recon_loss'], torch.Tensor) else loss_dict['recon_loss']
                    epoch_dag_loss += loss_dict['dag_constraint'].detach().item() if isinstance(loss_dict['dag_constraint'], torch.Tensor) else loss_dict['dag_constraint']
                    
                    # æ›´æ–°è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºå…³é”®æŸå¤±å’Œlogitsç»Ÿè®¡ä¿¡æ¯
                    logits_mean = causal_logits.detach().mean().item()
                    logits_std = causal_logits.detach().std().item()
                    train_pbar.set_postfix({
                        'Loss': f'{total_loss.detach().item():.4f}',
                        'Recon': f'{loss_dict["recon_loss"].detach().item() if isinstance(loss_dict["recon_loss"], torch.Tensor) else loss_dict["recon_loss"]:.4f}',
                        'DAG': f'{loss_dict["dag_constraint"].detach().item() if isinstance(loss_dict["dag_constraint"], torch.Tensor) else loss_dict["dag_constraint"]:.4f}',
                        'Logits_Mean': f'{logits_mean:.2f}',
                        'Logits_Std': f'{logits_std:.2f}'
                    })
                    
                except Exception as e:
                    print(f"âŒ è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
        
        # å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = epoch_train_loss / len(train_dataloader)
        avg_recon_loss = epoch_recon_loss / len(train_dataloader) # æ–°å¢ï¼šè®¡ç®—å¹³å‡é‡æ„æŸå¤±
        avg_dag_loss = epoch_dag_loss / len(train_dataloader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        epoch_val_loss = 0
        
        with torch.no_grad(), tqdm(val_dataloader, desc=f"Epoch {epoch+1}/{config['training']['num_epochs']} - Validation", 
                                  leave=False) as val_pbar:
            for batch_x, batch_y in val_pbar:
                try:
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    
                    # å‰å‘ä¼ æ’­
                    val_result = model(batch_x)
                    val_prediction_mean = val_result['prediction_mean']
                    val_prediction_log_var = val_result['prediction_log_var']
                    val_causal_matrix = val_result['causal_matrix']
                    val_causal_logits = val_result['causal_logits'] # è·å–logits
                    
                    # è®¡ç®—éªŒè¯æŸå¤± (åŒæ ·ä½¿ç”¨åŠ¨æ€æƒé‡)
                    _, val_loss_dict = model.get_loss(
                        val_prediction_mean,
                        val_prediction_log_var,
                        batch_y, 
                        val_causal_matrix, 
                        val_causal_logits, # ä¼ é€’logits
                        loss_weights=dynamic_loss_weights
                    )
                    
                    # ğŸ”§ val_loss_dict['total']å·²ç»æ˜¯YAMLæƒé‡è®¡ç®—çš„ç»“æœï¼Œç›´æ¥ä½¿ç”¨å¹¶detach
                    val_loss_value = val_loss_dict['total'].detach().item() if isinstance(val_loss_dict['total'], torch.Tensor) else val_loss_dict['total']
                    epoch_val_loss += val_loss_value
                    
                    val_pbar.set_postfix({'Val_Loss': f'{val_loss_value:.4f}'})
                    
                except Exception as e:
                    print(f"âŒ éªŒè¯æ‰¹æ¬¡å¤±è´¥: {e}")
                    continue
        
        avg_val_loss = epoch_val_loss / len(val_dataloader)
        val_losses.append(avg_val_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(avg_val_loss)
        else:
            scheduler.step() # CosineAnnealingWarmRestarts ç­‰å…¶ä»–è°ƒåº¦å™¨

        current_lr = optimizer.param_groups[0]['lr']
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss - config['training']['min_delta']:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(results_dir, 'best_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_val_loss,
                'config': config
            }, best_model_path)
            
        else:
            patience_counter += 1
        
        # æ”¹è¿›çš„è®­ç»ƒä¿¡æ¯æ˜¾ç¤º (å¢åŠ å½“å‰DAGæƒé‡)
        print(f"Epoch {epoch+1:3d}/{config['training']['num_epochs']:3d} | "
              f"Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f} | "
              f"Recon: {avg_recon_loss:.4f} | DAG: {avg_dag_loss:.4f} (W: {current_dag_weight:.4f}) | "
              f"LR: {current_lr:.6f} | Patience: {patience_counter}/{config['training']['patience']}")
        
        # è¯¦ç»†æŸå¤±ç»„ä»¶å’Œlogitsç»Ÿè®¡ï¼ˆæ¯5ä¸ªepochæ‰“å°ä¸€æ¬¡ï¼‰
        if (epoch + 1) % 5 == 0:
            print("   Loss Components & Logits Stats:")
            # åˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸ï¼ŒåªåŒ…å«æœ‰æ•°æ®çš„æŸå¤±ç»„ä»¶
            filtered_components = {k: v for k, v in loss_components.items() if v}
            for key, values in filtered_components.items():
                if values:
                    # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è·å–æœ€åä¸€ä¸ªå€¼
                    last_value = values[-1] if values else 0.0
                    print(f"      {key}: {last_value:.6f}")
            
            # æ‰“å°æœ€æ–°çš„logitsç»Ÿè®¡ä¿¡æ¯
            if 'causal_logits' in locals() and causal_logits is not None:
                logits_max = causal_logits.detach().max().item()
                logits_min = causal_logits.detach().min().item()
                print(f"      causal_logits_stats: Mean={logits_mean:.4f}, Std={logits_std:.4f}, Max={logits_max:.4f}, Min={logits_min:.4f}")

            # DAGçº¦æŸçŠ¶æ€æ£€æŸ¥
            dag_threshold = config.get('causal', {}).get('dag_loss_threshold', 0.1)
            if avg_dag_loss < dag_threshold:
                print(f"   ğŸ¯ DAGçº¦æŸå·²æ”¶æ•› (æŸå¤±: {avg_dag_loss:.6f} < é˜ˆå€¼: {dag_threshold})")
            else:
                print(f"   âš ï¸ DAGçº¦æŸä»åœ¨ä¼˜åŒ– (æŸå¤±: {avg_dag_loss:.6f} > é˜ˆå€¼: {dag_threshold})")
        
        # æ—©åœ
        if patience_counter >= config['training']['patience']:
            print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼Œè®­ç»ƒåœæ­¢ (patience: {config['training']['patience']})")
            break
        
        # æ¯ä¸ªepochçš„å¯è§†åŒ–å·²ç¦ç”¨ï¼Œä»…åœ¨è®­ç»ƒå®Œæˆåè¿›è¡Œå¯è§†åŒ–
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"   æ€»è®­ç»ƒè½®æ•°: {len(train_losses)}")
    
    # æŸå¤±è®°å½•å·²å®Œæˆï¼Œè¿”å›è®­ç»ƒç»“æœ
    print(f"\nâœ… è®­ç»ƒæ•°æ®å·²å‡†å¤‡å®Œæ¯•ï¼Œç­‰å¾…å¯è§†åŒ–å¤„ç†...")
    
    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'loss_components': loss_components,
        'best_val_loss': best_val_loss,
        'total_epochs': len(train_losses)
    }


def enforce_dag_constraint_postprocess(prob_matrix):
    """
    åå¤„ç†æ­¥éª¤ï¼šå¼ºåˆ¶DAGçº¦æŸï¼Œæ¶ˆé™¤åŒå‘è¾¹
    
    è§„åˆ™ï¼šå¦‚æœP(iâ†’j) > P(jâ†’i)ï¼Œåˆ™ä¿ç•™iâ†’jï¼Œå°†jâ†’iç½®ä¸º0
    
    Args:
        prob_matrix: æ¦‚ç‡çŸ©é˜µ [num_nodes, num_nodes]
        
    Returns:
        dag_matrix: å¼ºåˆ¶æ— ç¯çš„æ¦‚ç‡çŸ©é˜µ
        removed_edges: è¢«ç§»é™¤çš„è¾¹çš„ä¿¡æ¯
    """
    n_nodes = prob_matrix.shape[0]
    dag_matrix = prob_matrix.copy()
    removed_edges = []
    
    # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„åŒå‘è¾¹
    for i in range(n_nodes):
        for j in range(i + 1, n_nodes):  # åªæ£€æŸ¥ä¸Šä¸‰è§’ï¼Œé¿å…é‡å¤
            prob_ij = prob_matrix[i, j]  # P(i â†’ j)
            prob_ji = prob_matrix[j, i]  # P(j â†’ i)
            
            # å¦‚æœä¸¤ä¸ªæ–¹å‘éƒ½æœ‰æ˜¾è‘—æ¦‚ç‡ï¼ˆéƒ½å¤§äºæŸä¸ªå°é˜ˆå€¼ï¼‰
            if prob_ij > 1e-6 and prob_ji > 1e-6:
                # æ¯”è¾ƒä¸¤ä¸ªæ–¹å‘çš„æ¦‚ç‡ï¼Œä¿ç•™è¾ƒå¤§çš„ï¼Œç§»é™¤è¾ƒå°çš„
                if prob_ij > prob_ji:
                    # ä¿ç•™ i â†’ jï¼Œç§»é™¤ j â†’ i
                    dag_matrix[j, i] = 0
                    removed_edges.append(f"Removed edge {j}â†’{i} (prob={prob_ji:.6f}) due to stronger {i}â†’{j} (prob={prob_ij:.6f})")
                elif prob_ji > prob_ij:
                    # ä¿ç•™ j â†’ iï¼Œç§»é™¤ i â†’ j
                    dag_matrix[i, j] = 0
                    removed_edges.append(f"Removed edge {i}â†’{j} (prob={prob_ij:.6f}) due to stronger {j}â†’{i} (prob={prob_ji:.6f})")
                # å¦‚æœæ¦‚ç‡ç›¸ç­‰ï¼Œä¿æŒåŸçŠ¶ï¼ˆæ¦‚ç‡å¾ˆå°çš„æƒ…å†µä¸‹ä¸å¤ªå¯èƒ½å‘ç”Ÿï¼‰
    
    return dag_matrix, removed_edges


def analyze_causal_probabilities(model, test_data, config, results_dir):
    """åˆ†æå› æœå…³ç³»æ¦‚ç‡ - ä½¿ç”¨è’™ç‰¹å¡æ´›Dropoutä¼°è®¡ä¸ç¡®å®šæ€§"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.train()  # å¯ç”¨Dropoutä»¥è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡
    
    print("\n=== å› æœå…³ç³»æ¦‚ç‡åˆ†æ (è’™ç‰¹å¡æ´› Dropout) ===")
    
    n_samples = config['evaluation'].get('eval_samples', 100)
    prob_samples = []

    with torch.no_grad():
        for _ in tqdm(range(n_samples), desc="è’™ç‰¹å¡æ´›é‡‡æ ·", leave=False):
            result = model.forward(test_data)
            prob_samples.append(result['causal_matrix'])

    # å°†æ ·æœ¬å †å èµ·æ¥å¹¶è®¡ç®—å‡å€¼å’Œæ–¹å·®
    if not prob_samples:
        raise ValueError("è’™ç‰¹å¡æ´›é‡‡æ ·æœªèƒ½ç”Ÿæˆä»»ä½•æ¦‚ç‡æ ·æœ¬ã€‚")

    prob_stack = torch.stack(prob_samples, dim=0)
    
    # è®¡ç®—å‡å€¼ä½œä¸ºæœ€ç»ˆæ¦‚ç‡
    causal_probs_tensor = torch.mean(prob_stack, dim=0)
    
    # è®¡ç®—æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§
    uncertainty_tensor = torch.var(prob_stack, dim=0)

    # å°†æ¨¡å‹è®¾ç½®å›è¯„ä¼°æ¨¡å¼
    model.eval()

    # è½¬æ¢ä¸ºnumpyæ•°ç»„ - æ£€æŸ¥ç»´åº¦å¹¶æ­£ç¡®å¤„ç†
    if causal_probs_tensor.dim() == 4:
        causal_probs_tensor = causal_probs_tensor.mean(dim=1)
    
    if causal_probs_tensor.dim() == 3:
        causal_probs_raw_numpy = causal_probs_tensor[0].cpu().numpy()
        uncertainty_numpy = uncertainty_tensor[0].cpu().numpy()
    else:
        causal_probs_raw_numpy = causal_probs_tensor.cpu().numpy()
        uncertainty_numpy = uncertainty_tensor.cpu().numpy()

    print(f"\nåŸå§‹æ¦‚ç‡çŸ©é˜µèŒƒå›´: [{causal_probs_raw_numpy.min():.3f}, {causal_probs_raw_numpy.max():.3f}]")
    print(f"åŸå§‹æ¦‚ç‡çŸ©é˜µå‡å€¼: {causal_probs_raw_numpy.mean():.3f}")
    
    # åº”ç”¨åå¤„ç†
    causal_probs_final = causal_probs_raw_numpy.copy()
    removed_edges_info = "æœªåº”ç”¨DAGçº¦æŸåå¤„ç†"
    
    if config.get('probability_scaling', {}).get('enforce_dag_constraint', False):
        print("ğŸ”§ åº”ç”¨DAGçº¦æŸåå¤„ç†...")
        causal_probs_final, removed_edges = enforce_dag_constraint_postprocess(causal_probs_raw_numpy)
        if removed_edges:
            removed_edges_info = f"ç§»é™¤äº† {len(removed_edges)} æ¡å†²çªè¾¹"
            for edge_info in removed_edges[:5]:
                print(f"  - {edge_info}")
        else:
            removed_edges_info = "DAGçº¦æŸåå¤„ç†å®Œæˆï¼Œæœªå‘ç°å†²çªè¾¹"
    
    # åŸºäºæœ€ç»ˆçš„æ¦‚ç‡çŸ©é˜µï¼ˆå¯èƒ½ç»è¿‡åå¤„ç†ï¼‰é‡æ–°è®¡ç®—å› æœå›¾
    confidence_threshold = config['probabilistic']['confidence_threshold']
    uncertainty_threshold = config['probabilistic']['uncertainty_threshold']
    
    causal_graph_numpy = (causal_probs_final > confidence_threshold).astype(int)
    high_confidence_mask = uncertainty_numpy < uncertainty_threshold
    high_conf_graph_numpy = causal_graph_numpy * high_confidence_mask.astype(int)
    
    # æ‰“å°å› æœå…³ç³»æ¦‚ç‡
    num_nodes = causal_probs_final.shape[0]
    print("\næœ€ç»ˆå› æœå…³ç³»æ¦‚ç‡çŸ©é˜µ:")
    for i in range(num_nodes):
        for j in range(num_nodes):
            if i != j and causal_probs_final[i, j] > 0.01: # åªæ˜¾ç¤ºæœ‰ä¸€å®šå¯èƒ½æ€§çš„è¾¹
                prob = causal_probs_final[i, j]
                unc = uncertainty_numpy[i, j]
                print(f"P(Node{i} â†’ Node{j}) = {prob:.3f} Â± {unc:.3f}")
    
    print(f"\næ£€æµ‹åˆ°çš„å› æœå…³ç³»: {causal_graph_numpy.sum()} (é˜ˆå€¼ > {confidence_threshold})")
    print(f"é«˜ç½®ä¿¡åº¦å› æœå…³ç³»: {high_conf_graph_numpy.sum()} (ä¸ç¡®å®šæ€§ < {uncertainty_threshold})")
    
    # è¾“å‡ºæ¦‚ç‡é‚»æ¥çŸ©é˜µä¿¡æ¯
    print(f"\nğŸ“Š æœ€ç»ˆæ¦‚ç‡é‚»æ¥çŸ©é˜µä¿¡æ¯:")
    print(f"   - {removed_edges_info}")
    print(f"   - æœ€ç»ˆèŒƒå›´: [{causal_probs_final.min():.3f}, {causal_probs_final.max():.3f}]")
    print(f"   - æœ€ç»ˆå‡å€¼: {causal_probs_final.mean():.3f}")
    
    # ğŸ”§ ä¿å­˜ç»“æœ - æ–‡ä»¶åæ›´æ¸…æ™°
    pd.DataFrame(causal_probs_raw_numpy).to_csv(os.path.join(results_dir, 'causal_probabilities_raw.csv'))
    pd.DataFrame(causal_probs_final).to_csv(os.path.join(results_dir, 'causal_probabilities_final.csv'))
    pd.DataFrame(causal_graph_numpy).to_csv(os.path.join(results_dir, 'causal_graph.csv'))
    
    print("âœ… å·²ä¿å­˜åŸå§‹æ¦‚ç‡çŸ©é˜µåˆ° causal_probabilities_raw.csv")
    print("âœ… å·²ä¿å­˜æœ€ç»ˆæ¦‚ç‡çŸ©é˜µåˆ° causal_probabilities_final.csv")
    
    return causal_probs_final, uncertainty_numpy, causal_graph_numpy, high_conf_graph_numpy


def evaluate_against_ground_truth(causal_probs, causal_graph, high_conf_graph, ground_truth, config, results_dir, output_json=None):
    """ä¸çœŸå®é‚»æ¥çŸ©é˜µå¯¹æ¯”è¯„ä¼°ï¼ŒåŒ…å«AUROCå’Œè¯¦ç»†çš„é€é˜ˆå€¼æŒ‡æ ‡"""
    if ground_truth is None:
        print("è­¦å‘Š: æ²¡æœ‰çœŸå®é‚»æ¥çŸ©é˜µï¼Œè·³è¿‡è¯„ä¼°")
        if output_json:
            print("âŒ ä¼˜åŒ–æ¨¡å¼ä¸‹æ— çœŸå®çŸ©é˜µï¼Œæ— æ³•è¯„ä¼°ã€‚å°†è¿”å›F1=0.0å’ŒAUROC=0.5")
            metrics_to_save = {'best_f1_score': 0.0, 'auroc': 0.5, 'error': 'Ground truth not available.'}
            with open(output_json, 'w', encoding='utf-8') as f:
                json.dump(metrics_to_save, f, indent=4)
        return None
    
    print("\n=== ä¸çœŸå®é‚»æ¥çŸ©é˜µå¯¹æ¯”è¯„ä¼°ï¼ˆåŒ…å«AUROCå’Œè¯¦ç»†æŒ‡æ ‡ï¼‰ ===")
    
    # 1. è®¡ç®—ç²¾ç¡®çš„ROCæ›²çº¿å’ŒAUROC
    roc_thresholds = np.unique(causal_probs.ravel())
    if len(roc_thresholds) > 200:
        roc_thresholds = np.linspace(roc_thresholds.min(), roc_thresholds.max(), 200)
    
    all_fprs, all_tprs = [], []
    for threshold in roc_thresholds:
        pred_graph = (causal_probs > threshold).astype(int)
        metrics = evaluate_causal_discovery(pred_adj=pred_graph, true_adj=ground_truth)
        all_fprs.append(metrics['fpr'])
        all_tprs.append(metrics['tpr'])
        
    all_fprs, all_tprs = [0.0] + all_fprs + [1.0], [0.0] + all_tprs + [1.0]
    sorted_indices = np.argsort(all_fprs)
    sorted_fprs, sorted_tprs = np.array(all_fprs)[sorted_indices], np.array(all_tprs)[sorted_indices]
    roc_auc = auc(sorted_fprs, sorted_tprs)
    print(f"ğŸ“ˆ AUROC (Area Under ROC Curve): {roc_auc:.4f}")

    # 2. æ¢å¤å¯¹ä¸€ç»„å›ºå®šé˜ˆå€¼çš„è¯¦ç»†æŒ‡æ ‡æŠ¥å‘Š
    print("\n--- è¯¦ç»†é€é˜ˆå€¼æ€§èƒ½è¯„ä¼° ---")
    reporting_thresholds = np.arange(0.05, 1.0, 0.05)
    results = {}
    best_f1 = 0
    best_threshold_for_f1 = None
    # æ–°å¢ï¼šè·Ÿè¸ªæœ€ä½³SHD
    best_shd = float('inf')
    best_threshold_for_shd = None
    
    for threshold in reporting_thresholds:
        pred_graph = (causal_probs > threshold).astype(int)
        metrics = evaluate_causal_discovery(pred_adj=pred_graph, true_adj=ground_truth)
        results[f'threshold_{threshold:.2f}'] = metrics
        
        if metrics['f1'] > best_f1:
            best_f1 = metrics['f1']
            best_threshold_for_f1 = threshold
        
        # æ–°å¢ï¼šæ›´æ–°æœ€ä½³SHD
        if metrics['shd'] < best_shd:
            best_shd = metrics['shd']
            best_threshold_for_shd = threshold
            
        print(f"Threshold {threshold:.2f}: F1={metrics['f1']:.3f}, SHD={metrics['shd']:.0f}, Precision={metrics['precision']:.3f}, "
              f"Recall={metrics['recall']:.3f}, TPR={metrics['tpr']:.3f}, "
              f"FPR={metrics['fpr']:.3f}, Edges={pred_graph.sum()}")

    # 3. æ¢å¤å¯¹é»˜è®¤å›¾å’Œé«˜ç½®ä¿¡åº¦å›¾çš„è¯„ä¼°æ‰“å°
    print("\n--- é»˜è®¤é˜ˆå€¼ä¸é«˜ç½®ä¿¡åº¦å›¾æ€§èƒ½ ---")
    current_metrics = evaluate_causal_discovery(pred_adj=causal_graph, true_adj=ground_truth)
    results['current_causal_graph'] = current_metrics
    print(f"é»˜è®¤å›¾ (Thr={config['probabilistic']['confidence_threshold']:.2f}): F1={current_metrics['f1']:.3f}, SHD={current_metrics['shd']:.0f}, "
          f"Precision={current_metrics['precision']:.3f}, Recall={current_metrics['recall']:.3f}")

    high_conf_metrics = evaluate_causal_discovery(pred_adj=high_conf_graph, true_adj=ground_truth)
    results['high_confidence_graph'] = high_conf_metrics
    print(f"é«˜ç½®ä¿¡åº¦å›¾ (Uncertainty<{config['probabilistic']['uncertainty_threshold']:.2f}): F1={high_conf_metrics['f1']:.3f}, SHD={high_conf_metrics['shd']:.0f}, "
          f"Precision={high_conf_metrics['precision']:.3f}, Recall={high_conf_metrics['recall']:.3f}")
    
    print(f"\nğŸ¯ æœ€ä½³F1åˆ†æ•°: {best_f1:.3f} (åœ¨é˜ˆå€¼ = {best_threshold_for_f1:.2f} æ—¶å–å¾—)")
    print(f"ğŸ¯ æœ€ä½³SHD (è¶Šå°è¶Šå¥½): {best_shd:.0f} (åœ¨é˜ˆå€¼ = {best_threshold_for_shd:.2f} æ—¶å–å¾—)")
    
    # 4. ä¿å­˜è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
    report_path = os.path.join(results_dir, 'evaluation_results.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=== Probabilistic Causal Discovery Evaluation Results ===\n")
        f.write(f"AUROC: {roc_auc:.4f}\n")
        f.write(f"Best F1 Score: {best_f1:.4f} at threshold {best_threshold_for_f1:.2f}\n")
        f.write(f"Best SHD: {best_shd:.0f} at threshold {best_threshold_for_shd:.2f}\n\n")
        f.write("--- Detailed Metrics per Threshold ---\n")
        for key, metrics in results.items():
            if 'threshold' in key:
                f.write(f"{key}:\n")
                f.write(f"  F1={metrics['f1']:.4f}, SHD={metrics['shd']:.0f}, P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, "
                        f"TPR={metrics['tpr']:.4f}, FPR={metrics['fpr']:.4f}\n")
    print(f"âœ… è¯¦ç»†è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # 5. æ–°å¢ï¼šä¿å­˜ROCæ›²çº¿æ•°æ®ä»¥ä¾›åç»­å¯¹æ¯”ç»˜å›¾
    roc_data_path = os.path.join(results_dir, 'roc_data_lag.json')
    roc_data = {
        'model_name': 'EPC-PFNO', # æˆ–ä»é…ç½®ä¸­è¯»å–
        'fpr': sorted_fprs.tolist(),
        'tpr': sorted_tprs.tolist(),
        'auroc': roc_auc
    }
    with open(roc_data_path, 'w', encoding='utf-8') as f:
        json.dump(roc_data, f, indent=4)
    print(f"âœ… ROCæ›²çº¿æ•°æ®å·²ä¿å­˜: {roc_data_path}")

    # 6. ä¿å­˜å…³é”®æŒ‡æ ‡ç”¨äºè¶…å‚æ•°ä¼˜åŒ–
    if output_json:
        output_dir = os.path.dirname(output_json)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        metrics_to_save = {
            'best_f1_score': best_f1,
            'auroc': roc_auc,
            'best_shd': best_shd,
            'best_f1_threshold': best_threshold_for_f1,
            'best_shd_threshold': best_threshold_for_shd,
            'current_graph_metrics': results.get('current_causal_graph', {}),
        }
        
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(metrics_to_save, f, indent=4, cls=NpEncoder)
        print(f"âœ… ä¼˜åŒ–æŒ‡æ ‡å·²ä¿å­˜è‡³: {output_json}")

    return sorted_fprs, sorted_tprs, roc_auc, results


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®å®éªŒ
    args, config, results_dir, actual_seed = setup_experiment()
    
    # åŠ è½½æ•°æ®
    train_dataloader, val_dataloader, seq_length, num_nodes, ground_truth, raw_data = load_experimental_data(config)
    
    # åˆ›å»ºPFNOæ¨¡å‹
    model = create_probabilistic_model(num_nodes, seq_length, config)
    print("åˆ›å»ºPFNOæ¨¡å‹å®Œæˆ")
    
    # è®­ç»ƒæ¨¡å‹
    training_results = train_probabilistic_model(model, train_dataloader, val_dataloader, config, results_dir)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = next(iter(val_dataloader))[0][:1].to(next(model.parameters()).device)
    
    # åˆ†æå› æœæ¦‚ç‡
    causal_probs, uncertainty, causal_graph, high_conf_graph = analyze_causal_probabilities(
        model, test_data, config, results_dir
    )
    
    # ä½¿ç”¨ç»Ÿä¸€å¯è§†åŒ–å¥—ä»¶ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
    if not args.no_viz and PFNO_VIZ_AVAILABLE and PFNOVisualizationSuite is not None:
        print(f"\nğŸ“Š å¯åŠ¨PFNOå¯è§†åŒ–å¥—ä»¶...")
        viz_suite = PFNOVisualizationSuite(results_dir, config)
        
        # --- è°ƒè¯•ä»£ç ï¼šæ£€æŸ¥è¾“å…¥æ•°æ® ---
        print("\n" + "="*20 + " è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å¯è§†åŒ–è¾“å…¥ " + "="*20)
        print(f"   - [model] ç±»å‹: {type(model)}")
        print(f"   - [test_data] ç±»å‹: {type(test_data)}, å½¢çŠ¶: {test_data.shape if hasattr(test_data, 'shape') else 'N/A'}")
        if raw_data is not None:
            print(f"   - [raw_data] ç±»å‹: {type(raw_data)}, å½¢çŠ¶: {raw_data.shape if hasattr(raw_data, 'shape') else 'N/A'}")
            if hasattr(raw_data, 'dtype'):
                print(f"   - [raw_data] æ•°æ®ç±»å‹: {raw_data.dtype}")
            # æ£€æŸ¥åŸå§‹æ•°æ®æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if isinstance(raw_data, np.ndarray):
                print(f"   - [raw_data] NaNæ•°é‡: {np.isnan(raw_data).sum()}, Infæ•°é‡: {np.isinf(raw_data).sum()}")
        else:
            print(f"   - [raw_data] ä¸º None! è¿™æ˜¯å¯¼è‡´æ—¶é—´åºåˆ—å¯è§†åŒ–å¤±è´¥çš„ç›´æ¥åŸå› ã€‚")
        print("="*68 + "\n")
        # --- è°ƒè¯•ä»£ç ç»“æŸ ---

        # è¿è¡Œæ‰€æœ‰å¯è§†åŒ–ä»»åŠ¡
        success_count, total_tasks = viz_suite.run_all_visualizations(
            train_losses=training_results['train_losses'],
            val_losses=training_results['val_losses'],
            loss_components=training_results['loss_components'],
            predicted_matrix=causal_probs,
            true_matrix=ground_truth,
            model=model,
            test_data=test_data,
            raw_data=raw_data
        )
        
        print(f"\nğŸ¯ å¯è§†åŒ–ä»»åŠ¡å®Œæˆ: {success_count}/{total_tasks} ä¸ªä»»åŠ¡æˆåŠŸ")
    else:
        if args.no_viz:
            print("\nğŸš« å·²é€šè¿‡å‘½ä»¤è¡Œç¦ç”¨å¯è§†åŒ–ã€‚")
        else:
            print(f"\nâš ï¸ å¯è§†åŒ–å¥—ä»¶ä¸å¯ç”¨ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
            print(f"ğŸ’¡ å¦‚éœ€å¯è§†åŒ–åŠŸèƒ½ï¼Œè¯·æ£€æŸ¥ pfno/utils/visualization.py æ¨¡å—")
    
    # è¯„ä¼°æ€§èƒ½ï¼ˆåŒ…å«AUROCå’ŒTPR/FPRï¼‰
    evaluation_outputs = evaluate_against_ground_truth(
        causal_probs, causal_graph, high_conf_graph, ground_truth, config, results_dir,
        output_json=args.output_json  # ä¼ é€’output_jsonè·¯å¾„
    )
    
    # å¦‚æœè¯„ä¼°æˆåŠŸï¼Œåˆ™è¿›è¡Œå¯è§†åŒ–
    if not args.no_viz and evaluation_outputs:
        sorted_fprs, sorted_tprs, roc_auc, _ = evaluation_outputs
        # ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å‘½åï¼Œä»¥åœ¨å›¾ä¾‹ä¸­æ˜¾ç¤º
        model_name = "EPC-PFNO"
        plot_roc_curve(sorted_fprs, sorted_tprs, roc_auc, model_name, results_dir)
    
    print(f"\n=== å®éªŒå®Œæˆ ===")
    print(f"ç»“æœä¿å­˜åœ¨: {results_dir}")
    print("ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
    print("ğŸ“Š è¯„ä¼°ç»“æœ:")
    print("- evaluation_results.txt: è¯„ä¼°ç»“æœï¼ˆåŒ…å«TPRå’ŒFPRæŒ‡æ ‡ï¼‰")
    print("- causal_probabilities_raw.csv: åŸå§‹å› æœå…³ç³»æ¦‚ç‡çŸ©é˜µ")
    print("- causal_probabilities_final.csv: ç¼©æ”¾åçš„å› æœå…³ç³»æ¦‚ç‡çŸ©é˜µ")
    print("- causal_graph.csv: äºŒå€¼åŒ–å› æœå›¾")
    print("ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨ï¼ˆç”±PFNOå¯è§†åŒ–å¥—ä»¶ç”Ÿæˆï¼‰:")
    if PFNO_VIZ_AVAILABLE and PFNOVisualizationSuite is not None:
        print("- training_curves_final.png: è®­ç»ƒè¿‡ç¨‹æ›²çº¿ï¼ˆåŒ…å«å¹³æ»‘å¤„ç†ï¼‰")
        print("- adjacency_matrix_comparison.png: é‚»æ¥çŸ©é˜µå¯¹æ¯”å›¾")
        print("- time_series_reconstruction_detailed.png: å•èŠ‚ç‚¹è¯¦ç»†é‡æ„åˆ†æ")
        print("- time_series_reconstruction_overview.png: å¤šèŠ‚ç‚¹é‡æ„æ¦‚è§ˆ")
        print("- time_series_reconstruction_grid.png: ç½‘æ ¼åŒ–é‡æ„åˆ†æ")
        print("- reconstruction_statistics.txt: æ—¶é—´åºåˆ—é‡æ„æ€§èƒ½ç»Ÿè®¡")
    
    # æ·»åŠ ROCæ›²çº¿åˆ°è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
    print("- roc_curve.png: æ–°å¢çš„ROCæ›²çº¿å›¾")
    
    print("âš™ï¸ é…ç½®æ–‡ä»¶:")
    print("- experiment_config.json: å®éªŒé…ç½®æ–‡ä»¶")


if __name__ == "__main__":
    main()