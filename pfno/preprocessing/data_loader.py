"""
PFNOå¢å¼ºæ•°æ®åŠ è½½å™¨

é›†æˆäº†æ–°çš„é¢„å¤„ç†ç³»ç»Ÿï¼Œæ”¯æŒå››ç§ä¸åŒçº§åˆ«çš„æ•°æ®é¢„å¤„ç†
"""

import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, TensorDataset
import pickle
from sklearn.preprocessing import StandardScaler

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
from .core import preprocess_pfno_data, TimeSeriesPreprocessor


class TimeSeriesAugmentation(torch.utils.data.Dataset):
    """æ—¶é—´åºåˆ—æ•°æ®å¢å¼ºç±»"""
    
    def __init__(self, dataset, num_nodes, sequence_length, config=None):
        self.dataset = dataset
        self.num_nodes = num_nodes
        self.sequence_length = sequence_length
        self.original_size = len(dataset)
        
        # ä»é…ç½®ä¸­è·å–å¢å¼ºå‚æ•°ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
        if config and 'augmentation' in config:
            self.augment_factor = config['augmentation'].get('factor', 1.5)
            self.techniques = config['augmentation'].get('techniques', ['scaling'])
            self.noise_levels = config['augmentation'].get('noise_level', [0.01, 0.05])
            self.scale_range = config['augmentation'].get('scale_range', [0.8, 1.2])
            self.time_warp_width = config['augmentation'].get('time_warp_width', [1, 5])
            self.window_shift = config['augmentation'].get('window_shift', [-2, 2])
        else:
            # é»˜è®¤é…ç½®
            self.augment_factor = 1.5
            self.techniques = ['scaling']
            self.noise_levels = [0.01, 0.05]
            self.scale_range = [0.8, 1.2]
            self.time_warp_width = [1, 5]
            self.window_shift = [-2, 2]
        
        # è®¡ç®—æ€»å¤§å°
        self.total_size = int(self.original_size * self.augment_factor)
        
    def __len__(self):
        return self.total_size
    
    def __getitem__(self, idx):
        # å¦‚æœç´¢å¼•åœ¨åŸå§‹èŒƒå›´å†…ï¼Œè¿”å›åŸå§‹æ•°æ®
        if idx < self.original_size:
            return self.dataset[idx]
        
        # å¦åˆ™è¿”å›å¢å¼ºæ•°æ®
        original_idx = idx % self.original_size
        inputs, targets = self.dataset[original_idx]
        
        # éšæœºé€‰æ‹©å¢å¼ºæŠ€æœ¯
        technique = np.random.choice(self.techniques)
        
        if technique == 'noise':
            noise_level = np.random.uniform(self.noise_levels[0], self.noise_levels[1])
            noise = torch.randn_like(inputs) * noise_level
            inputs = inputs + noise
            targets = targets + noise
            
        elif technique == 'scaling':
            scale_factor = np.random.uniform(self.scale_range[0], self.scale_range[1])
            inputs = inputs * scale_factor
            targets = targets * scale_factor
            
        elif technique == 'time_warp':
            # ç®€å•çš„æ—¶é—´æ‰­æ›²
            warp_width = np.random.randint(self.time_warp_width[0], self.time_warp_width[1] + 1)
            if warp_width < inputs.shape[-1]:
                start_idx = np.random.randint(0, inputs.shape[-1] - warp_width)
                warp_factor = np.random.uniform(0.8, 1.2)
                inputs[:, start_idx:start_idx + warp_width] *= warp_factor
                targets[:, start_idx:start_idx + warp_width] *= warp_factor
                
        elif technique == 'window_shift':
            shift = np.random.randint(self.window_shift[0], self.window_shift[1] + 1)
            if shift != 0:
                inputs = torch.roll(inputs, shifts=shift, dims=-1)
                targets = torch.roll(targets, shifts=shift, dims=-1)
        
        return inputs, targets


def load_pfno_data(config):
    """
    ä¸ºPFNOåŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    
    Args:
        config: PFNOé…ç½®å­—å…¸
        
    Returns:
        train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        seq_length: åºåˆ—é•¿åº¦
        num_nodes: èŠ‚ç‚¹æ•°é‡
        preprocessor: é¢„å¤„ç†å™¨å®ä¾‹
        raw_data: åŸå§‹æ•°æ®
        preprocessing_info: é¢„å¤„ç†ä¿¡æ¯
    """
    data_config = config['data']
    preprocessing_config = data_config['preprocessing']
    
    print(f"\n{'='*60}")
    print(f"              PFNOæ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
    print(f"{'='*60}")
    
    # 1. è¯»å–åŸå§‹æ•°æ®
    file_path = data_config['file_path']
    print(f"ğŸ“‚ è¯»å–æ•°æ®æ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    raw_data = pd.read_csv(file_path)
    print(f"ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶: {raw_data.shape}")
    
    # ä¿å­˜åŸå§‹æ•°æ®å‰¯æœ¬
    original_data = raw_data.copy()
    
    # 2. é™åˆ¶æ—¶é—´æ­¥æ•°
    max_time_steps = data_config.get('max_time_steps')
    if max_time_steps and max_time_steps < raw_data.shape[0]:
        raw_data = raw_data.iloc[:max_time_steps, :]
        print(f"ğŸ”¢ é™åˆ¶ä¸ºå‰{max_time_steps}ä¸ªæ—¶é—´æ­¥ï¼Œå®é™…ä½¿ç”¨æ•°æ®å½¢çŠ¶: {raw_data.shape}")
    
    # 3. æ‰§è¡Œé¢„å¤„ç†
    print(f"\nğŸ”§ é¢„å¤„ç†æ–¹æ³•: {preprocessing_config['method']}")
    processed_data, preprocessor, preprocessing_info = preprocess_pfno_data(
        raw_data, preprocessing_config
    )
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆ")
    for step in preprocessing_info['steps']:
        print(f"   â€¢ {step}")
    
    # 4. ç”Ÿæˆæ—¶é—´åºåˆ—æ ·æœ¬
    sequence_length = data_config['sequence_length']
    num_nodes = processed_data.shape[1]
    
    print(f"\nğŸ“Š æ•°æ®ç»´åº¦ä¿¡æ¯:")
    print(f"   â€¢ èŠ‚ç‚¹æ•°: {num_nodes}")
    print(f"   â€¢ åºåˆ—é•¿åº¦: {sequence_length}")
    print(f"   â€¢ æ—¶é—´æ­¥æ•°: {len(processed_data)}")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    data_values = processed_data.values
    
    # ç”Ÿæˆåºåˆ—æ ·æœ¬
    samples = []
    targets = []
    
    for i in range(len(data_values) - sequence_length):
        sample = data_values[i:i + sequence_length]  # è¾“å…¥åºåˆ—
        target = data_values[i + 1:i + sequence_length + 1]  # ç›®æ ‡åºåˆ—ï¼ˆä¸‹ä¸€æ—¶åˆ»ï¼‰
        samples.append(sample)
        targets.append(target)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è°ƒæ•´ç»´åº¦
    x = np.array(samples)  # [num_samples, sequence_length, num_nodes]
    y = np.array(targets)  # [num_samples, sequence_length, num_nodes]
    
    # è°ƒæ•´ç»´åº¦ä¸º (num_samples, num_nodes, sequence_length)
    x = x.transpose(0, 2, 1)
    y = y.transpose(0, 2, 1)
    
    print(f"   â€¢ ç”Ÿæˆæ ·æœ¬æ•°: {x.shape[0]}")
    print(f"   â€¢ æ ·æœ¬å½¢çŠ¶: {x.shape}")
    
    # 5. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_ratio = data_config['train_ratio']
    train_size = int(x.shape[0] * train_ratio)
    
    x_train, x_val = x[:train_size], x[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]
    
    print(f"   â€¢ è®­ç»ƒé›†å¤§å°: {len(x_train)}")
    print(f"   â€¢ éªŒè¯é›†å¤§å°: {len(x_val)}")
    
    # 6. å±•å¹³æ•°æ®ï¼ˆPFNOéœ€è¦çš„æ ¼å¼ï¼‰
    x_train = x_train.reshape(-1, num_nodes * sequence_length)
    y_train = y_train.reshape(-1, num_nodes * sequence_length)
    x_val = x_val.reshape(-1, num_nodes * sequence_length)
    y_val = y_val.reshape(-1, num_nodes * sequence_length)
    
    # 7. è½¬æ¢ä¸ºPyTorchå¼ é‡
    x_train = torch.tensor(x_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32)
    x_val = torch.tensor(x_val, dtype=torch.float32)
    y_val = torch.tensor(y_val, dtype=torch.float32)
    
    # 8. åˆ›å»ºæ•°æ®é›†
    train_dataset = TensorDataset(x_train, y_train)
    val_dataset = TensorDataset(x_val, y_val)
    
    # 9. æ•°æ®å¢å¼ºï¼ˆä»…å¯¹è®­ç»ƒé›†ï¼‰
    if data_config.get('augment', False):
        print(f"\nğŸ”„ åº”ç”¨æ•°æ®å¢å¼º...")
        augmented_train_dataset = TimeSeriesAugmentation(
            train_dataset, num_nodes, sequence_length, data_config
        )
        print(f"   â€¢ åŸå§‹è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"   â€¢ å¢å¼ºåè®­ç»ƒé›†å¤§å°: {len(augmented_train_dataset)}")
        train_dataset = augmented_train_dataset
    
    # 10. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = config['training']['batch_size']
    
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=0  # åœ¨æŸäº›ç¯å¢ƒä¸­é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=0
    )
    
    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ!")
    print(f"   â€¢ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_dataloader)}")
    print(f"   â€¢ éªŒè¯æ‰¹æ¬¡æ•°: {len(val_dataloader)}")
    print(f"{'='*60}\n")
    
    return (train_dataloader, val_dataloader, sequence_length, num_nodes, 
            preprocessor, original_data, preprocessing_info)


def load_experimental_data_enhanced(config):
    """
    å¢å¼ºç‰ˆçš„å®éªŒæ•°æ®åŠ è½½å‡½æ•°ï¼Œä½¿ç”¨æ–°çš„é¢„å¤„ç†ç³»ç»Ÿ
    
    æ›¿æ¢åŸæ¥çš„load_experimental_dataå‡½æ•°
    """
    # ä½¿ç”¨æ–°çš„æ•°æ®åŠ è½½å™¨
    (train_dataloader, val_dataloader, seq_length, num_nodes, 
     preprocessor, raw_data, preprocessing_info) = load_pfno_data(config)
    
    # åŠ è½½çœŸå®é‚»æ¥çŸ©é˜µ
    ground_truth_file = config['data']['ground_truth_file']
    ground_truth = None
    
    if os.path.exists(ground_truth_file):
        try:
            ground_truth_df = pd.read_csv(ground_truth_file, index_col=0, header=0)
            ground_truth = ground_truth_df.values
            print(f"ğŸ“Š çœŸå®é‚»æ¥çŸ©é˜µå½¢çŠ¶: {ground_truth.shape}")
            print(f"ğŸ“Š çœŸå®é‚»æ¥çŸ©é˜µèŠ‚ç‚¹æ ‡ç­¾: {ground_truth_df.columns.tolist()}")
            
            # éªŒè¯çŸ©é˜µç»´åº¦åŒ¹é…
            if ground_truth.shape[0] != num_nodes or ground_truth.shape[1] != num_nodes:
                print(f"âš ï¸  è­¦å‘Š: çœŸå®é‚»æ¥çŸ©é˜µç»´åº¦ {ground_truth.shape} ä¸æ•°æ®èŠ‚ç‚¹æ•° {num_nodes} ä¸åŒ¹é…")
                if ground_truth.shape[0] == ground_truth.shape[1]:
                    print(f"å°†ä½¿ç”¨ {ground_truth.shape[0]}x{ground_truth.shape[0]} çš„çœŸå®é‚»æ¥çŸ©é˜µ")
                    num_nodes = ground_truth.shape[0]
                else:
                    print("çœŸå®é‚»æ¥çŸ©é˜µä¸æ˜¯æ–¹é˜µï¼Œå°†è®¾ç½®ä¸ºNone")
                    ground_truth = None
            
            if ground_truth is not None:
                print(f"ğŸ“Š çœŸå®é‚»æ¥çŸ©é˜µä¸­çš„è¾¹æ•°: {ground_truth.sum()}")
                
        except Exception as e:
            print(f"âŒ åŠ è½½çœŸå®é‚»æ¥çŸ©é˜µæ—¶å‡ºé”™: {e}")
            ground_truth = None
    else:
        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°çœŸå®é‚»æ¥çŸ©é˜µæ–‡ä»¶ {ground_truth_file}")
    
    # ä¿å­˜é¢„å¤„ç†å™¨å’Œé¢„å¤„ç†ä¿¡æ¯
    results_dir = config.get('output', {}).get('results_base_dir', 'results')
    os.makedirs(results_dir, exist_ok=True)
    
    # ä¿å­˜é¢„å¤„ç†å™¨
    preprocessor_path = os.path.join(results_dir, 'preprocessor.pkl')
    with open(preprocessor_path, 'wb') as f:
        pickle.dump(preprocessor, f)
    print(f"ğŸ’¾ é¢„å¤„ç†å™¨å·²ä¿å­˜åˆ°: {preprocessor_path}")
    
    # ä¿å­˜é¢„å¤„ç†ä¿¡æ¯
    preprocessing_info_path = os.path.join(results_dir, 'preprocessing_info.pkl')
    with open(preprocessing_info_path, 'wb') as f:
        pickle.dump(preprocessing_info, f)
    print(f"ğŸ’¾ é¢„å¤„ç†ä¿¡æ¯å·²ä¿å­˜åˆ°: {preprocessing_info_path}")
    
    return train_dataloader, val_dataloader, seq_length, num_nodes, ground_truth, raw_data 