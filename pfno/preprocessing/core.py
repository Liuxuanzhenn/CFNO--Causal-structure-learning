"""
PFNOæ•°æ®é¢„å¤„ç†æ¨¡å—

æä¾›å››ç§ä¸åŒçº§åˆ«çš„æ—¶é—´åºåˆ—æ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼š
1. minimal - æœ€å°é¢„å¤„ç†ï¼šä»…å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
2. standard - æ ‡å‡†é¢„å¤„ç†ï¼ˆæ¨èï¼‰ï¼šåŒ…å«åŸºæœ¬æ¸…ç† + Z-scoreæ ‡å‡†åŒ–
3. comprehensive - å…¨é¢é¢„å¤„ç†ï¼šåŒ…å«æ‰€æœ‰é¢„å¤„ç†æ­¥éª¤ï¼ŒåŒ…æ‹¬å¼‚å¸¸å€¼å¤„ç†å’Œå¹³ç¨³æ€§æ£€æŸ¥
4. time_series_focused - æ—¶é—´åºåˆ—ä¸“ç”¨ï¼šä¸“ä¸ºæ—¶é—´åºåˆ—æ•°æ®è®¾è®¡ï¼ŒåŒ…å«æ’å€¼ã€å»è¶‹åŠ¿ç­‰
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.ensemble import IsolationForest
from scipy import signal, interpolate, stats
from scipy.signal import butter, filtfilt, detrend as scipy_detrend
import warnings
warnings.filterwarnings('ignore')


class TimeSeriesPreprocessor:
    """æ—¶é—´åºåˆ—æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            config: é¢„å¤„ç†é…ç½®å­—å…¸
        """
        self.config = config
        self.scalers = {}
        self.preprocessing_stats = {}
        
    def preprocess(self, data, method='standard'):
        """
        æ ¹æ®æŒ‡å®šæ–¹æ³•é¢„å¤„ç†æ•°æ®
        
        Args:
            data: pandas.DataFrame, åŸå§‹æ—¶é—´åºåˆ—æ•°æ®
            method: str, é¢„å¤„ç†æ–¹æ³•
            
        Returns:
            processed_data: é¢„å¤„ç†åçš„æ•°æ®
            preprocessing_info: é¢„å¤„ç†ä¿¡æ¯
        """
        if method not in self.config:
            raise ValueError(f"ä¸æ”¯æŒçš„é¢„å¤„ç†æ–¹æ³•: {method}")
            
        method_config = self.config[method]
        processing_info = {'method': method, 'steps': []}
        
        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        processed_data = data.copy()
        
        print(f"\nğŸ”§ å¼€å§‹ '{method}' é¢„å¤„ç†...")
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {processed_data.shape}")
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        if method_config.get('handle_missing', False):
            processed_data, missing_info = self._handle_missing_values(
                processed_data, method_config
            )
            processing_info['steps'].append(f"å¤„ç†ç¼ºå¤±å€¼: {missing_info}")
            
        # 2. å¤„ç†å¼‚å¸¸å€¼
        if method_config.get('handle_outliers', False):
            processed_data, outlier_info = self._handle_outliers(
                processed_data, method_config
            )
            processing_info['steps'].append(f"å¤„ç†å¼‚å¸¸å€¼: {outlier_info}")
            
        # 3. æ—¶é—´åºåˆ—ä¸“ç”¨é¢„å¤„ç†
        if method == 'time_series_focused':
            processed_data = self._time_series_specific_preprocessing(
                processed_data, method_config
            )
            processing_info['steps'].append("æ—¶é—´åºåˆ—ä¸“ç”¨é¢„å¤„ç†")
            
        # 4. å…¨é¢é¢„å¤„ç†çš„é¢å¤–æ­¥éª¤
        if method == 'comprehensive':
            processed_data = self._comprehensive_preprocessing(
                processed_data, method_config
            )
            processing_info['steps'].append("å…¨é¢é¢„å¤„ç†æ­¥éª¤")
            
        # 5. æ•°æ®æ ‡å‡†åŒ–
        if method_config.get('normalize', False):
            processed_data, norm_info = self._normalize_data(
                processed_data, method_config
            )
            processing_info['steps'].append(f"æ•°æ®æ ‡å‡†åŒ–: {norm_info}")
            
        print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {processed_data.shape}")
        print(f"é¢„å¤„ç†æ­¥éª¤: {len(processing_info['steps'])} ä¸ª")
        
        return processed_data, processing_info
    
    def _handle_missing_values(self, data, config):
        """å¤„ç†ç¼ºå¤±å€¼"""
        missing_count = data.isnull().sum().sum()
        if missing_count == 0:
            return data, "æ— ç¼ºå¤±å€¼"
            
        method = config.get('missing_method', 'interpolate')
        
        if method == 'interpolate':
            data = data.interpolate(method='linear', limit_direction='both')
        elif method == 'spline':
            # æ—¶é—´åºåˆ—ä¸“ç”¨çš„æ ·æ¡æ’å€¼
            order = config.get('spline_order', 3)
            for col in data.columns:
                if data[col].isnull().any():
                    # è·å–éç©ºå€¼çš„ç´¢å¼•å’Œå€¼
                    not_null = data[col].notnull()
                    if not_null.sum() > order:
                        x = data.index[not_null]
                        y = data[col][not_null]
                        # åˆ›å»ºæ ·æ¡æ’å€¼å™¨
                        f = interpolate.interp1d(x, y, kind='cubic', 
                                               fill_value='extrapolate')
                        # æ’å€¼æ‰€æœ‰ç¼ºå¤±å€¼
                        data[col] = data[col].fillna(pd.Series(f(data.index), index=data.index))
        elif method == 'forward_fill':
            data = data.fillna(method='ffill')
        elif method == 'backward_fill':
            data = data.fillna(method='bfill')
        elif method == 'mean':
            data = data.fillna(data.mean())
            
        return data, f"å¤„ç†äº† {missing_count} ä¸ªç¼ºå¤±å€¼ï¼Œæ–¹æ³•: {method}"
    
    def _handle_outliers(self, data, config):
        """å¤„ç†å¼‚å¸¸å€¼"""
        method = config.get('outlier_method', 'iqr')
        threshold = config.get('outlier_threshold', 3.0)
        outlier_count = 0
        
        for col in data.columns:
            if method == 'iqr':
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR
                outliers = (data[col] < lower_bound) | (data[col] > upper_bound)
                
            elif method == 'zscore':
                z_scores = np.abs(stats.zscore(data[col]))
                outliers = z_scores > threshold
                
            elif method == 'isolation':
                if len(data) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬
                    iso_forest = IsolationForest(contamination=0.1, random_state=42)
                    outliers = iso_forest.fit_predict(data[col].values.reshape(-1, 1)) == -1
                else:
                    outliers = np.zeros(len(data), dtype=bool)
            
            # è®°å½•å¼‚å¸¸å€¼æ•°é‡
            outlier_count += outliers.sum()
            
            # å¤„ç†å¼‚å¸¸å€¼ï¼ˆç”¨ä¸­ä½æ•°æ›¿æ¢ï¼‰
            data.loc[outliers, col] = data[col].median()
            
        # é¢å¤–çš„æˆªæ–­å¤„ç†
        if config.get('clip_outliers', False):
            percentiles = config.get('clip_percentiles', [1, 99])
            for col in data.columns:
                lower_p = np.percentile(data[col], percentiles[0])
                upper_p = np.percentile(data[col], percentiles[1])
                data[col] = data[col].clip(lower=lower_p, upper=upper_p)
            
        return data, f"å¤„ç†äº† {outlier_count} ä¸ªå¼‚å¸¸å€¼ï¼Œæ–¹æ³•: {method}"
    
    def _time_series_specific_preprocessing(self, data, config):
        """æ—¶é—´åºåˆ—ä¸“ç”¨é¢„å¤„ç†"""
        # å»è¶‹åŠ¿
        if config.get('detrend', False):
            detrend_method = config.get('detrend_method', 'linear')
            for col in data.columns:
                if detrend_method == 'linear':
                    data[col] = scipy_detrend(data[col], type='linear')
                elif detrend_method == 'constant':
                    data[col] = scipy_detrend(data[col], type='constant')
        
        # åº”ç”¨æ»¤æ³¢å™¨
        if config.get('apply_filter', False):
            filter_type = config.get('filter_type', 'butterworth')
            
            if filter_type == 'butterworth':
                order = config.get('filter_order', 4)
                cutoff = config.get('filter_cutoff', 0.1)
                
                # è®¾è®¡å·´ç‰¹æ²ƒæ€ä½é€šæ»¤æ³¢å™¨
                b, a = butter(order, cutoff, btype='low')
                
                for col in data.columns:
                    # åº”ç”¨é›¶ç›¸ä½æ»¤æ³¢
                    data[col] = filtfilt(b, a, data[col])
                    
            elif filter_type == 'moving_average':
                window = config.get('smoothing_window', 3)
                data = data.rolling(window=window, center=True).mean()
                data = data.fillna(method='bfill').fillna(method='ffill')
        
        return data
    
    def _comprehensive_preprocessing(self, data, config):
        """å…¨é¢é¢„å¤„ç†çš„é¢å¤–æ­¥éª¤"""
        # æ£€æŸ¥å¹³ç¨³æ€§
        if config.get('check_stationarity', False):
            try:
                from statsmodels.tsa.stattools import adfuller
                
                for col in data.columns:
                    # ADFæ£€éªŒ
                    try:
                        result = adfuller(data[col].dropna())
                        is_stationary = result[1] <= 0.05  # p-value <= 0.05 è¡¨ç¤ºå¹³ç¨³
                        
                        # å¦‚æœä¸å¹³ç¨³ä¸”éœ€è¦å¤„ç†
                        if not is_stationary and config.get('make_stationary', False):
                            max_diff = config.get('max_diff_order', 2)
                            
                            # å°è¯•å·®åˆ†
                            diff_data = data[col].copy()
                            for i in range(max_diff):
                                diff_data = diff_data.diff().dropna()
                                if len(diff_data) > 10:
                                    result = adfuller(diff_data)
                                    if result[1] <= 0.05:
                                        break
                            
                            # å¦‚æœå·®åˆ†åå¹³ç¨³ï¼Œä½¿ç”¨å·®åˆ†æ•°æ®ï¼ˆä½†éœ€è¦é‡æ–°æ„é€ åŸé•¿åº¦ï¼‰
                            if result[1] <= 0.05:
                                # ç®€å•åœ°ç”¨å·®åˆ†åçš„æ•°æ®å¡«å……
                                data[col] = data[col].diff().fillna(0)
                                
                    except Exception as e:
                        print(f"å¹³ç¨³æ€§æ£€éªŒå¤±è´¥ {col}: {e}")
            except ImportError:
                print("è­¦å‘Š: statsmodelsæœªå®‰è£…ï¼Œè·³è¿‡å¹³ç¨³æ€§æ£€éªŒ")
        
        # å»é™¤è¶‹åŠ¿
        if config.get('remove_trend', False):
            trend_method = config.get('trend_method', 'linear')
            
            for col in data.columns:
                if trend_method == 'linear':
                    # çº¿æ€§å»è¶‹åŠ¿
                    x = np.arange(len(data))
                    coeffs = np.polyfit(x, data[col], 1)
                    trend = np.polyval(coeffs, x)
                    data[col] = data[col] - trend
                elif trend_method == 'polynomial':
                    # å¤šé¡¹å¼å»è¶‹åŠ¿
                    x = np.arange(len(data))
                    coeffs = np.polyfit(x, data[col], 2)
                    trend = np.polyval(coeffs, x)
                    data[col] = data[col] - trend
        
        # å¹³æ»‘æ•°æ®
        if config.get('smooth_data', False):
            window = config.get('smoothing_window', 3)
            data = data.rolling(window=window, center=True).mean()
            data = data.fillna(method='bfill').fillna(method='ffill')
        
        return data
    
    def _normalize_data(self, data, config):
        """æ•°æ®æ ‡å‡†åŒ–"""
        method = config.get('normalization_method', 'zscore')
        
        normalized_data = data.copy()
        
        for col in data.columns:
            if method == 'zscore':
                scaler = StandardScaler()
            elif method == 'minmax':
                scaler = MinMaxScaler()
            elif method == 'robust':
                scaler = RobustScaler()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {method}")
            
            # æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®
            col_data = data[col].values.reshape(-1, 1)
            normalized_data[col] = scaler.fit_transform(col_data).flatten()
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            self.scalers[col] = scaler
        
        return normalized_data, f"æ ‡å‡†åŒ–æ–¹æ³•: {method}"
    
    def inverse_transform(self, data, columns=None):
        """åæ ‡å‡†åŒ–æ•°æ®"""
        if not self.scalers:
            return data
            
        if columns is None:
            columns = data.columns if hasattr(data, 'columns') else range(data.shape[1])
        
        denormalized_data = data.copy()
        
        for i, col in enumerate(columns):
            if col in self.scalers:
                if hasattr(data, 'iloc'):
                    col_data = data.iloc[:, i].values.reshape(-1, 1)
                    denormalized_data.iloc[:, i] = self.scalers[col].inverse_transform(col_data).flatten()
                else:
                    col_data = data[:, i].reshape(-1, 1)
                    denormalized_data[:, i] = self.scalers[col].inverse_transform(col_data).flatten()
        
        return denormalized_data


def preprocess_pfno_data(data, preprocessing_config):
    """
    PFNOæ•°æ®é¢„å¤„ç†çš„ä¾¿æ·å‡½æ•°
    
    Args:
        data: pandas.DataFrame, åŸå§‹æ•°æ®
        preprocessing_config: dict, é¢„å¤„ç†é…ç½®
        
    Returns:
        processed_data: é¢„å¤„ç†åçš„æ•°æ®
        preprocessor: é¢„å¤„ç†å™¨å®ä¾‹ï¼ˆç”¨äºåç»­åå˜æ¢ï¼‰
        info: é¢„å¤„ç†ä¿¡æ¯
    """
    method = preprocessing_config.get('method', 'standard')
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = TimeSeriesPreprocessor(preprocessing_config)
    
    # æ‰§è¡Œé¢„å¤„ç†
    processed_data, info = preprocessor.preprocess(data, method)
    
    return processed_data, preprocessor, info


# é¢„è®¾é…ç½®ç¤ºä¾‹
PRESET_PREPROCESSING_CONFIGS = {
    'minimal': {
        'handle_missing': True,
        'handle_outliers': True,
        'outlier_method': 'iqr',
        'outlier_threshold': 3.0,
        'missing_method': 'interpolate',
        'normalize': False
    },
    
    'standard': {
        'handle_missing': True,
        'handle_outliers': True,
        'outlier_method': 'iqr',
        'outlier_threshold': 3.0,
        'missing_method': 'interpolate',
        'normalize': True,
        'normalization_method': 'zscore',
        'clip_outliers': True,
        'clip_percentiles': [1, 99]
    },
    
    'comprehensive': {
        'handle_missing': True,
        'handle_outliers': True,
        'outlier_method': 'isolation',
        'outlier_threshold': 2.5,
        'missing_method': 'interpolate',
        'normalize': True,
        'normalization_method': 'robust',
        'clip_outliers': True,
        'clip_percentiles': [0.5, 99.5],
        'check_stationarity': True,
        'make_stationary': True,
        'max_diff_order': 2,
        'remove_trend': True,
        'trend_method': 'linear',
        'smooth_data': True,
        'smoothing_window': 3
    },
    
    'time_series_focused': {
        'handle_missing': True,
        'missing_method': 'spline',
        'spline_order': 3,
        'handle_outliers': True,
        'outlier_method': 'zscore',
        'outlier_threshold': 2.5,
        'normalize': True,
        'normalization_method': 'zscore',
        'detrend': True,
        'detrend_method': 'linear',
        'seasonal_decompose': False,
        'seasonal_period': None,
        'apply_filter': True,
        'filter_type': 'butterworth',
        'filter_order': 4,
        'filter_cutoff': 0.1,
        'interpolate_method': 'cubic',
        'handle_edge_effects': True
    }
} 