"""
å¯è§†åŒ–å·¥å…·æ¨¡å—
æä¾›PFNOå®éªŒç»“æœçš„å¯è§†åŒ–åŠŸèƒ½
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import warnings
import torch
import os
import time

# è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®seabornæ ·å¼
sns.set_style("whitegrid")
sns.set_palette("husl")


def plot_adjacency_matrix(adj_matrix: np.ndarray, 
                         title: str = "é‚»æ¥çŸ©é˜µ",
                         save_path: Optional[str] = None,
                         **kwargs) -> None:
    """
    ç»˜åˆ¶é‚»æ¥çŸ©é˜µçƒ­åŠ›å›¾
    
    Args:
        adj_matrix: é‚»æ¥çŸ©é˜µ
        title: å›¾è¡¨æ ‡é¢˜
        save_path: ä¿å­˜è·¯å¾„
        **kwargs: å…¶ä»–ç»˜å›¾å‚æ•°
    """
    fig, ax = plt.subplots(figsize=kwargs.get('figsize', (8, 6)))
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    im = ax.imshow(adj_matrix, cmap=kwargs.get('cmap', 'RdBu_r'), 
                   vmin=kwargs.get('vmin', 0), vmax=kwargs.get('vmax', 1))
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel('ç›®æ ‡èŠ‚ç‚¹', fontsize=12)
    ax.set_ylabel('æºèŠ‚ç‚¹', fontsize=12)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('è¿æ¥å¼ºåº¦', fontsize=10)
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼ˆå¦‚æœçŸ©é˜µä¸å¤ªå¤§ï¼‰
    if adj_matrix.shape[0] <= 10 and kwargs.get('annotate', True):
        for i in range(adj_matrix.shape[0]):
            for j in range(adj_matrix.shape[1]):
                text = ax.text(j, i, f'{adj_matrix[i, j]:.2f}',
                             ha="center", va="center", color="black", fontsize=8)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()


def plot_training_curves(training_history: Dict[str, List], 
                        save_path: Optional[str] = None,
                        **kwargs) -> None:
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    
    Args:
        training_history: è®­ç»ƒå†å²å­—å…¸
        save_path: ä¿å­˜è·¯å¾„
        **kwargs: å…¶ä»–ç»˜å›¾å‚æ•°
    """
    fig, axes = plt.subplots(2, 2, figsize=kwargs.get('figsize', (15, 10)))
    
    # ä¸»æŸå¤±æ›²çº¿
    if 'train_losses' in training_history and 'val_losses' in training_history:
        axes[0, 0].plot(training_history['train_losses'], label='Training Loss', alpha=0.8)
        axes[0, 0].plot(training_history['val_losses'], label='Validation Loss', alpha=0.8)
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
    
    # æŸå¤±ç»„ä»¶
    if 'loss_components' in training_history:
        for key, values in training_history['loss_components'].items():
            if values and any(v > 0 for v in values):
                axes[0, 1].plot(values, label=key, alpha=0.8)
        axes[0, 1].set_title('Loss Components')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss Value')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡æ›²çº¿
    if 'learning_rates' in training_history:
        axes[1, 0].plot(training_history['learning_rates'], alpha=0.8)
        axes[1, 0].set_title('Learning Rate Schedule')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
    
    # æ€§èƒ½æŒ‡æ ‡
    if 'metrics' in training_history:
        metrics = training_history['metrics']
        for metric_name, values in metrics.items():
            if isinstance(values, list) and values:
                axes[1, 1].plot(values, label=metric_name, alpha=0.8)
        axes[1, 1].set_title('Performance Metrics')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Metric Value')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Training curves saved to: {save_path}")
    
    plt.show()


def plot_evaluation_metrics(metrics: Dict[str, float],
                           save_path: Optional[str] = None,
                           **kwargs) -> None:
    """
    ç»˜åˆ¶è¯„ä¼°æŒ‡æ ‡æ¡å½¢å›¾
    
    Args:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        save_path: ä¿å­˜è·¯å¾„
        **kwargs: å…¶ä»–ç»˜å›¾å‚æ•°
    """
    # è¿‡æ»¤æ‰éæ•°å€¼æŒ‡æ ‡
    numeric_metrics = {k: v for k, v in metrics.items() 
                      if isinstance(v, (int, float)) and not k.startswith('t')}
    
    if not numeric_metrics:
        print("æ²¡æœ‰å¯ç»˜åˆ¶çš„æ•°å€¼æŒ‡æ ‡")
        return
    
    fig, ax = plt.subplots(figsize=kwargs.get('figsize', (10, 6)))
    
    metric_names = list(numeric_metrics.keys())
    metric_values = list(numeric_metrics.values())
    
    bars = ax.bar(range(len(metric_names)), metric_values, 
                  alpha=0.8, color=sns.color_palette("husl", len(metric_names)))
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel('è¯„ä¼°æŒ‡æ ‡', fontsize=12)
    ax.set_ylabel('æŒ‡æ ‡å€¼', fontsize=12)
    ax.set_title('æ¨¡å‹è¯„ä¼°ç»“æœ', fontsize=14, fontweight='bold')
    ax.set_xticks(range(len(metric_names)))
    ax.set_xticklabels(metric_names, rotation=45, ha='right')
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i, (bar, value) in enumerate(zip(bars, metric_values)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.3f}', ha='center', va='bottom', fontsize=10)
    
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"è¯„ä¼°æŒ‡æ ‡å›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()


def plot_comparison_results(results_list: List[Dict[str, Any]],
                           experiment_names: List[str],
                           metrics: List[str] = None,
                           save_path: Optional[str] = None,
                           **kwargs) -> None:
    """
    ç»˜åˆ¶å¤šä¸ªå®éªŒç»“æœå¯¹æ¯”å›¾
    
    Args:
        results_list: å®éªŒç»“æœåˆ—è¡¨
        experiment_names: å®éªŒåç§°åˆ—è¡¨
        metrics: è¦æ¯”è¾ƒçš„æŒ‡æ ‡åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        **kwargs: å…¶ä»–ç»˜å›¾å‚æ•°
    """
    if metrics is None:
        metrics = ['f1', 'precision', 'recall', 'accuracy']
    
    # æå–æ•°æ®
    data = []
    for i, (results, exp_name) in enumerate(zip(results_list, experiment_names)):
        if 'evaluation' in results:
            eval_results = results['evaluation']
            for metric in metrics:
                if metric in eval_results:
                    data.append({
                        'experiment': exp_name,
                        'metric': metric,
                        'value': eval_results[metric]
                    })
    
    if not data:
        print("æ²¡æœ‰å¯æ¯”è¾ƒçš„æ•°æ®")
        return
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(data)
    
    # ç»˜åˆ¶åˆ†ç»„æ¡å½¢å›¾
    fig, ax = plt.subplots(figsize=kwargs.get('figsize', (12, 6)))
    
    sns.barplot(data=df, x='metric', y='value', hue='experiment', ax=ax)
    
    ax.set_title('å®éªŒç»“æœå¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xlabel('è¯„ä¼°æŒ‡æ ‡', fontsize=12)
    ax.set_ylabel('æŒ‡æ ‡å€¼', fontsize=12)
    ax.legend(title='å®éªŒ', bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()


def plot_threshold_analysis(threshold_results: Dict[str, List],
                           save_path: Optional[str] = None,
                           **kwargs) -> None:
    """
    ç»˜åˆ¶é˜ˆå€¼åˆ†æå›¾
    
    Args:
        threshold_results: é˜ˆå€¼åˆ†æç»“æœ
        save_path: ä¿å­˜è·¯å¾„
        **kwargs: å…¶ä»–ç»˜å›¾å‚æ•°
    """
    fig, axes = plt.subplots(2, 2, figsize=kwargs.get('figsize', (12, 8)))
    
    thresholds = threshold_results.get('thresholds', [])
    
    # F1åˆ†æ•° vs é˜ˆå€¼
    if 'f1' in threshold_results:
        axes[0, 0].plot(thresholds, threshold_results['f1'], 'o-', alpha=0.8)
        axes[0, 0].set_title('F1åˆ†æ•° vs é˜ˆå€¼')
        axes[0, 0].set_xlabel('é˜ˆå€¼')
        axes[0, 0].set_ylabel('F1åˆ†æ•°')
        axes[0, 0].grid(True, alpha=0.3)
    
    # ç²¾ç¡®ç‡å’Œå¬å›ç‡ vs é˜ˆå€¼
    if 'precision' in threshold_results and 'recall' in threshold_results:
        axes[0, 1].plot(thresholds, threshold_results['precision'], 'o-', 
                       label='ç²¾ç¡®ç‡', alpha=0.8)
        axes[0, 1].plot(thresholds, threshold_results['recall'], 's-', 
                       label='å¬å›ç‡', alpha=0.8)
        axes[0, 1].set_title('ç²¾ç¡®ç‡å’Œå¬å›ç‡ vs é˜ˆå€¼')
        axes[0, 1].set_xlabel('é˜ˆå€¼')
        axes[0, 1].set_ylabel('æŒ‡æ ‡å€¼')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡ vs é˜ˆå€¼
    if 'accuracy' in threshold_results:
        axes[1, 0].plot(thresholds, threshold_results['accuracy'], '^-', alpha=0.8)
        axes[1, 0].set_title('å‡†ç¡®ç‡ vs é˜ˆå€¼')
        axes[1, 0].set_xlabel('é˜ˆå€¼')
        axes[1, 0].set_ylabel('å‡†ç¡®ç‡')
        axes[1, 0].grid(True, alpha=0.3)
    
    # ç»“æ„æ±‰æ˜è·ç¦» vs é˜ˆå€¼
    if 'shd' in threshold_results:
        axes[1, 1].plot(thresholds, threshold_results['shd'], 'd-', alpha=0.8)
        axes[1, 1].set_title('ç»“æ„æ±‰æ˜è·ç¦» vs é˜ˆå€¼')
        axes[1, 1].set_xlabel('é˜ˆå€¼')
        axes[1, 1].set_ylabel('SHD')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"é˜ˆå€¼åˆ†æå›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()


def create_experiment_dashboard(results: Dict[str, Any],
                               config: Dict[str, Any],
                               save_dir: str,
                               experiment_name: str = "experiment") -> str:
    """
    åˆ›å»ºå®éªŒç»“æœä»ªè¡¨æ¿
    
    Args:
        results: å®éªŒç»“æœ
        config: é…ç½®ä¿¡æ¯
        save_dir: ä¿å­˜ç›®å½•
        experiment_name: å®éªŒåç§°
        
    Returns:
        ä¿å­˜ç›®å½•è·¯å¾„
    """
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“Š æ­£åœ¨åˆ›å»ºå®éªŒä»ªè¡¨æ¿...")
    
    # 1. å› æœçŸ©é˜µå¯è§†åŒ–
    if 'causal_matrix' in results:
        causal_matrix = results['causal_matrix']
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # æ¦‚ç‡çŸ©é˜µ
        im1 = axes[0].imshow(causal_matrix, cmap='RdBu_r', vmin=0, vmax=1)
        axes[0].set_title('å› æœæ¦‚ç‡çŸ©é˜µ')
        axes[0].set_xlabel('ç›®æ ‡èŠ‚ç‚¹')
        axes[0].set_ylabel('æºèŠ‚ç‚¹')
        plt.colorbar(im1, ax=axes[0])
        
        # äºŒå€¼åŒ–çŸ©é˜µ
        threshold = results.get('evaluation', {}).get('best_threshold', 0.5)
        binary_matrix = (causal_matrix > threshold).astype(int)
        im2 = axes[1].imshow(binary_matrix, cmap='RdBu_r', vmin=0, vmax=1)
        axes[1].set_title(f'äºŒå€¼åŒ–å› æœå›¾ (é˜ˆå€¼={threshold:.2f})')
        axes[1].set_xlabel('ç›®æ ‡èŠ‚ç‚¹')
        axes[1].set_ylabel('æºèŠ‚ç‚¹')
        plt.colorbar(im2, ax=axes[1])
        
        plt.tight_layout()
        causal_path = save_path / f"{experiment_name}_causal_matrices.png"
        plt.savefig(causal_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    # 2. è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
    if 'training' in results:
        plot_training_curves(results['training'], 
                            save_path=save_path / f"{experiment_name}_training.png")
        plt.close()
    
    # 3. è¯„ä¼°æŒ‡æ ‡å¯è§†åŒ–
    if 'evaluation' in results:
        plot_evaluation_metrics(results['evaluation'],
                               save_path=save_path / f"{experiment_name}_metrics.png")
        plt.close()
    
    print(f"âœ… å®éªŒä»ªè¡¨æ¿å·²åˆ›å»º: {save_path}")
    return str(save_path)


def save_plots(save_dir: str, **plots) -> None:
    """
    æ‰¹é‡ä¿å­˜å›¾è¡¨
    
    Args:
        save_dir: ä¿å­˜ç›®å½•
        **plots: å›¾è¡¨å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶åï¼Œå€¼ä¸ºå›¾è¡¨å¯¹è±¡
    """
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    
    for filename, fig in plots.items():
        if hasattr(fig, 'savefig'):
            file_path = save_path / f"{filename}.png"
            fig.savefig(file_path, dpi=300, bbox_inches='tight')
            print(f"å›¾è¡¨å·²ä¿å­˜: {file_path}")
        else:
            print(f"è­¦å‘Š: {filename} ä¸æ˜¯æœ‰æ•ˆçš„å›¾è¡¨å¯¹è±¡")


def plot_probability_matrix_comparison(predicted_prob_matrix, true_causal_matrix, 
                                     save_path=None, title_prefix="", 
                                     threshold=0.5, cmap='Blues'):
    """
    ç»˜åˆ¶æ¦‚ç‡çŸ©é˜µï¼ˆç¼©æ”¾åï¼‰ä¸çœŸå®å› æœçŸ©é˜µçš„å¯¹æ¯”å›¾
    
    Args:
        predicted_prob_matrix: é¢„æµ‹çš„æ¦‚ç‡çŸ©é˜µ (ç¼©æ”¾å)
        true_causal_matrix: çœŸå®å› æœçŸ©é˜µ
        save_path: ä¿å­˜è·¯å¾„
        title_prefix: æ ‡é¢˜å‰ç¼€
        threshold: äºŒå€¼åŒ–é˜ˆå€¼ï¼ˆç”¨äºç”Ÿæˆé¢„æµ‹çš„äºŒå€¼å›¾ï¼‰
        cmap: é¢œè‰²æ˜ å°„
    """
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if isinstance(predicted_prob_matrix, torch.Tensor):
        predicted_prob_matrix = predicted_prob_matrix.detach().cpu().numpy()
    if isinstance(true_causal_matrix, torch.Tensor):
        true_causal_matrix = true_causal_matrix.detach().cpu().numpy()
    
    # å¤„ç†ç»´åº¦
    if predicted_prob_matrix.ndim > 2:
        predicted_prob_matrix = predicted_prob_matrix.squeeze()
    if true_causal_matrix.ndim > 2:
        true_causal_matrix = true_causal_matrix.squeeze()
    
    # ç”ŸæˆäºŒå€¼åŒ–çš„é¢„æµ‹çŸ©é˜µ
    predicted_binary = (predicted_prob_matrix > threshold).astype(float)
    
    # è®¡ç®—å·®å¼‚çŸ©é˜µï¼ˆç”¨äºé”™è¯¯åˆ†æï¼‰
    diff_matrix = np.abs(predicted_binary - true_causal_matrix)
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle(f'{title_prefix}Probability Matrix vs Ground Truth Comparison', fontsize=16, y=0.98)
    
    # å­å›¾1: çœŸå®å› æœçŸ©é˜µ
    im1 = axes[0, 0].imshow(true_causal_matrix, cmap='Reds', vmin=0, vmax=1)
    axes[0, 0].set_title('Ground Truth Causal Matrix')
    axes[0, 0].set_xlabel('Target Node')
    axes[0, 0].set_ylabel('Source Node')
    plt.colorbar(im1, ax=axes[0, 0], fraction=0.046, pad=0.04)
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼ˆé€‚ç”¨äºå°çŸ©é˜µï¼‰
    n_nodes = true_causal_matrix.shape[0]
    if n_nodes <= 10:
        for i in range(n_nodes):
            for j in range(n_nodes):
                text = axes[0, 0].text(j, i, f'{true_causal_matrix[i, j]:.0f}',
                                     ha="center", va="center", color="white" if true_causal_matrix[i, j] > 0.5 else "black")
    
    # å­å›¾2: é¢„æµ‹æ¦‚ç‡çŸ©é˜µï¼ˆç¼©æ”¾åï¼‰
    im2 = axes[0, 1].imshow(predicted_prob_matrix, cmap=cmap, vmin=0, vmax=1)
    axes[0, 1].set_title(f'Predicted Probability Matrix (Scaled)')
    axes[0, 1].set_xlabel('Target Node')
    axes[0, 1].set_ylabel('Source Node')
    plt.colorbar(im2, ax=axes[0, 1], fraction=0.046, pad=0.04)
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼ˆé€‚ç”¨äºå°çŸ©é˜µï¼‰
    if n_nodes <= 10:
        for i in range(n_nodes):
            for j in range(n_nodes):
                text = axes[0, 1].text(j, i, f'{predicted_prob_matrix[i, j]:.2f}',
                                     ha="center", va="center", 
                                     color="white" if predicted_prob_matrix[i, j] > 0.5 else "black",
                                     fontsize=8)
    
    # å­å›¾3: é¢„æµ‹äºŒå€¼çŸ©é˜µï¼ˆé˜ˆå€¼åŒ–åï¼‰
    im3 = axes[1, 0].imshow(predicted_binary, cmap='Greens', vmin=0, vmax=1)
    axes[1, 0].set_title(f'Predicted Binary Matrix (Threshold={threshold})')
    axes[1, 0].set_xlabel('Target Node')
    axes[1, 0].set_ylabel('Source Node')
    plt.colorbar(im3, ax=axes[1, 0], fraction=0.046, pad=0.04)
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    if n_nodes <= 10:
        for i in range(n_nodes):
            for j in range(n_nodes):
                text = axes[1, 0].text(j, i, f'{predicted_binary[i, j]:.0f}',
                                     ha="center", va="center", 
                                     color="white" if predicted_binary[i, j] > 0.5 else "black")
    
    # å­å›¾4: é”™è¯¯çŸ©é˜µï¼ˆå·®å¼‚åˆ†æï¼‰
    im4 = axes[1, 1].imshow(diff_matrix, cmap='Oranges', vmin=0, vmax=1)
    axes[1, 1].set_title('Error Matrix (|Predicted - Truth|)')
    axes[1, 1].set_xlabel('Target Node')
    axes[1, 1].set_ylabel('Source Node')
    plt.colorbar(im4, ax=axes[1, 1], fraction=0.046, pad=0.04)
    
    # æ ‡è®°é”™è¯¯ä½ç½®
    if n_nodes <= 10:
        for i in range(n_nodes):
            for j in range(n_nodes):
                if diff_matrix[i, j] > 0:
                    axes[1, 1].text(j, i, 'âœ—', ha="center", va="center", 
                                   color="red", fontsize=12, fontweight='bold')
                else:
                    axes[1, 1].text(j, i, 'âœ“', ha="center", va="center", 
                                   color="green", fontsize=10)
    
    # è®¡ç®—å’Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    try:
        from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
        
        true_flat = true_causal_matrix.flatten()
        pred_flat = predicted_binary.flatten()
        
        accuracy = accuracy_score(true_flat, pred_flat)
        precision = precision_score(true_flat, pred_flat, zero_division=0)
        recall = recall_score(true_flat, pred_flat, zero_division=0)
        f1 = f1_score(true_flat, pred_flat, zero_division=0)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
        stats_text = f"""Evaluation Metrics:
Accuracy: {accuracy:.3f}
Precision: {precision:.3f}
Recall: {recall:.3f}
F1 Score: {f1:.3f}
Error Edges: {int(np.sum(diff_matrix))}
Threshold: {threshold}"""
        
        plt.figtext(0.02, 0.02, stats_text, fontsize=10, 
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    except ImportError:
        print("Warning: sklearn not installed, skipping evaluation metrics calculation")
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout(rect=[0, 0.08, 1, 0.96])
    
    # ä¿å­˜æˆ–æ˜¾ç¤º
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Probability matrix comparison saved to: {save_path}")
    
    plt.show()


def plot_time_series_reconstruction(original_data, reconstructed_data,
                                  selected_nodes=None, max_timesteps=200,
                                  save_path=None, title_prefix="Time Series Reconstruction - "):
    """
    Plots a comparison of original and reconstructed time series data.
    Optimized for handling long sequences and providing clear visual feedback.
    
    Args:
        original_data (np.ndarray): Original time series data [timesteps, nodes].
        reconstructed_data (np.ndarray): Reconstructed time series data [timesteps, nodes].
        selected_nodes (list, optional): List of node indices to visualize. Defaults to auto-selection.
        max_timesteps (int, optional): Maximum number of timesteps to display. Defaults to 200.
        save_path (str, optional): Path to save the plot. Defaults to None (shows plot).
        title_prefix (str, optional): Prefix for the plot title.
    """
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if isinstance(original_data, torch.Tensor):
        original_data = original_data.detach().cpu().numpy()
    if isinstance(reconstructed_data, torch.Tensor):
        reconstructed_data = reconstructed_data.detach().cpu().numpy()
    
    # å¤„ç†æ‰¹æ¬¡ç»´åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if original_data.ndim == 3:
        original_data = original_data[0]
    if reconstructed_data.ndim == 3:
        reconstructed_data = reconstructed_data[0]
    
    # ç¡®ä¿æ•°æ®å½¢çŠ¶æ­£ç¡® [timesteps, nodes]
    if original_data.ndim != 2 or reconstructed_data.ndim != 2:
        print(f"âš ï¸ æ•°æ®ç»´åº¦å¼‚å¸¸ï¼Œè·³è¿‡å¯è§†åŒ–")
        return
    
    # é™åˆ¶æ—¶é—´æ­¥æ•°ä¸ºç”¨æˆ·è¦æ±‚çš„200
    timesteps = min(original_data.shape[0], reconstructed_data.shape[0], max_timesteps)
    original_data = original_data[:timesteps]
    reconstructed_data = reconstructed_data[:timesteps]
    
    n_nodes = min(original_data.shape[1], reconstructed_data.shape[1])
    original_data = original_data[:, :n_nodes]
    reconstructed_data = reconstructed_data[:, :n_nodes]
    
    print(f"ğŸ“Š å¯è§†åŒ–æ•°æ®: {timesteps}æ—¶é—´æ­¥ Ã— {n_nodes}èŠ‚ç‚¹")
    
    # è‡ªåŠ¨é€‰æ‹©è¦æ˜¾ç¤ºçš„èŠ‚ç‚¹ï¼ˆé’ˆå¯¹CSVæ•°æ®ä¼˜åŒ–ï¼‰
    if selected_nodes is None:
        # é€‰æ‹©æ–¹å·®æœ€å¤§çš„å‰6ä¸ªèŠ‚ç‚¹ï¼ˆCSVæ•°æ®æœ‰10ä¸ªèŠ‚ç‚¹ï¼‰
        node_variances = np.var(original_data, axis=0)
        top_variance_nodes = np.argsort(node_variances)[-6:]  # é€‰æ‹©æ–¹å·®æœ€å¤§çš„6ä¸ªèŠ‚ç‚¹
        selected_nodes = sorted(top_variance_nodes)  # æŒ‰ç´¢å¼•æ’åº
    
    # é™åˆ¶é€‰æ‹©çš„èŠ‚ç‚¹æ•°é‡
    selected_nodes = [node for node in selected_nodes if 0 <= node < n_nodes]
    n_selected = min(len(selected_nodes), 6)  # æœ€å¤šæ˜¾ç¤º6ä¸ªèŠ‚ç‚¹
    selected_nodes = selected_nodes[:n_selected]
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(n_selected, 1, figsize=(16, 4 * n_selected), constrained_layout=True)
    if n_selected == 1:
        axes = [axes]
    
    fig.suptitle(f'{title_prefix}Comparison (First {timesteps} Timesteps)', fontsize=18, fontweight='bold')
    
    # ä¸ºæ¯ä¸ªé€‰ä¸­çš„èŠ‚ç‚¹ç»˜åˆ¶å¯¹æ¯”å›¾
    time_axis = np.arange(timesteps)
    
    for i, node_idx in enumerate(selected_nodes):
        ax = axes[i]
        
        # åŸå§‹æ•°æ®
        ax.plot(time_axis, original_data[:, node_idx], 
               label=f'Original Data', color='#2E86C1', linewidth=2.5, alpha=0.9)
        
        # é‡æ„æ•°æ®
        ax.plot(time_axis, reconstructed_data[:, node_idx], 
               label=f'Reconstructed Data', color='#E74C3C', linewidth=2, alpha=0.9, linestyle='--')
        
        # è®¡ç®—é‡æ„è¯¯å·®æŒ‡æ ‡
        mse = np.mean((original_data[:, node_idx] - reconstructed_data[:, node_idx]) ** 2)
        mae = np.mean(np.abs(original_data[:, node_idx] - reconstructed_data[:, node_idx]))
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        if np.std(original_data[:, node_idx]) > 0 and np.std(reconstructed_data[:, node_idx]) > 0:
            correlation = np.corrcoef(original_data[:, node_idx], reconstructed_data[:, node_idx])[0, 1]
            if np.isnan(correlation):
                correlation = 0.0
        else:
            correlation = 0.0
        
        ax.set_title(f'Node {node_idx + 1} | MSE: {mse:.4f}, MAE: {mae:.4f}, Correlation: {correlation:.3f}', 
                    fontsize=14)
        ax.set_xlabel('Time Steps', fontsize=12)
        ax.set_ylabel('Value', fontsize=12)
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.4, linestyle=':')
        
        # æ·»åŠ è¯¯å·®å¡«å……åŒºåŸŸï¼ˆæµ…è‰²ï¼‰
        ax.fill_between(time_axis, 
                       original_data[:, node_idx], 
                       reconstructed_data[:, node_idx],
                       alpha=0.15, color='orange')
        
        # è®¾ç½®åæ ‡è½´æ ·å¼
        ax.tick_params(axis='both', which='major', labelsize=10)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout(rect=[0, 0.05, 1, 0.95])
    
    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    overall_mse = np.mean((original_data[:, selected_nodes] - reconstructed_data[:, selected_nodes]) ** 2)
    overall_mae = np.mean(np.abs(original_data[:, selected_nodes] - reconstructed_data[:, selected_nodes]))
    
    # æ·»åŠ æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
    stats_text = f"""Overall Performance (for {n_selected} selected nodes):
    MSE: {overall_mse:.6f} | MAE: {overall_mae:.6f}
    Displayed Nodes: {[f'Node {i+1}' for i in selected_nodes]}
    Time Window: 1-{timesteps} (out of {original_data.shape[0]} total steps)"""
    
    fig.text(0.5, -0.01, stats_text, fontsize=11, ha='center',
             bbox=dict(boxstyle="round,pad=0.5", facecolor="#F0F0F0", alpha=0.8))
    
    # ä¿å­˜æˆ–æ˜¾ç¤º
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… Time series reconstruction plot saved to: {save_path}")
    
    plt.close(fig)


def plot_comprehensive_results(model_result, original_data, true_causal_matrix,
                             selected_nodes=None, max_timesteps=100,
                             threshold=0.5, save_dir=None, title_prefix=""):
    """
    ç»˜åˆ¶ç»¼åˆç»“æœå¯è§†åŒ–ï¼ˆåŒ…æ‹¬æ¦‚ç‡çŸ©é˜µå¯¹æ¯”å’Œæ—¶é—´åºåˆ—é‡æ„ï¼‰
    
    Args:
        model_result: æ¨¡å‹å‰å‘ä¼ æ’­ç»“æœå­—å…¸ï¼ŒåŒ…å« 'prediction_samples', 'adj_probs' ç­‰
        original_data: åŸå§‹æ—¶é—´åºåˆ—æ•°æ®
        true_causal_matrix: çœŸå®å› æœçŸ©é˜µ
        selected_nodes: è¦æ˜¾ç¤ºçš„èŠ‚ç‚¹åˆ—è¡¨
        max_timesteps: æœ€å¤§æ—¶é—´æ­¥æ•°
        threshold: æ¦‚ç‡çŸ©é˜µäºŒå€¼åŒ–é˜ˆå€¼
        save_dir: ä¿å­˜ç›®å½•
        title_prefix: æ ‡é¢˜å‰ç¼€
    """
    import os
    
    # å‡†å¤‡æ•°æ®
    predicted_prob_matrix = model_result.get('adj_probs')
    prediction_samples = model_result.get('prediction_samples')
    
    # å¤„ç†é‡æ„æ•°æ®
    if prediction_samples is not None:
        # å–é¢„æµ‹æ ·æœ¬çš„å‡å€¼ä½œä¸ºé‡æ„æ•°æ®
        if isinstance(prediction_samples, torch.Tensor):
            reconstructed_data = prediction_samples.mean(dim=-1).detach().cpu().numpy()
        else:
            reconstructed_data = np.mean(prediction_samples, axis=-1)
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ°é¢„æµ‹æ ·æœ¬æ•°æ®ï¼Œè·³è¿‡æ—¶é—´åºåˆ—é‡æ„å›¾")
        reconstructed_data = None
    
    # 1. ç»˜åˆ¶æ¦‚ç‡çŸ©é˜µå¯¹æ¯”å›¾
    if predicted_prob_matrix is not None and true_causal_matrix is not None:
        prob_save_path = None
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            prob_save_path = os.path.join(save_dir, f"{title_prefix}probability_matrix_comparison.png")
        
        plot_probability_matrix_comparison(
            predicted_prob_matrix, true_causal_matrix,
            save_path=prob_save_path, title_prefix=title_prefix,
            threshold=threshold
        )
    
    # 2. ç»˜åˆ¶æ—¶é—´åºåˆ—é‡æ„å›¾
    if reconstructed_data is not None and original_data is not None:
        # ç¡®ä¿æ•°æ®å½¢çŠ¶å…¼å®¹
        try:
            # é‡æ„æ•°æ®éœ€è¦reshapeä¸ºæ—¶é—´åºåˆ—æ ¼å¼
            if reconstructed_data.ndim == 2:
                # [batch_size, features] -> [timesteps, nodes]
                batch_size, features = reconstructed_data.shape
                # å‡è®¾æ•°æ®æ˜¯ [batch_size, sequence_length * num_nodes] æ ¼å¼
                if hasattr(model_result.get('model', None), 'num_nodes'):
                    num_nodes = model_result['model'].num_nodes
                    sequence_length = features // num_nodes
                elif true_causal_matrix is not None:
                    num_nodes = true_causal_matrix.shape[0]
                    sequence_length = features // num_nodes
                else:
                    # ä¼°ç®—èŠ‚ç‚¹æ•°ï¼ˆå‡è®¾æ˜¯æ–¹é˜µçš„è¾¹æ•°ï¼‰
                    num_nodes = int(np.sqrt(features))
                    sequence_length = features // num_nodes
                
                reconstructed_data = reconstructed_data[0].reshape(sequence_length, num_nodes)
            
            recon_save_path = None
            if save_dir:
                recon_save_path = os.path.join(save_dir, f"{title_prefix}time_series_reconstruction.png")
            
            plot_time_series_reconstruction(
                original_data, reconstructed_data,
                selected_nodes=selected_nodes, max_timesteps=max_timesteps,
                save_path=recon_save_path, title_prefix=title_prefix
            )
            
        except Exception as e:
            print(f"è­¦å‘Š: æ—¶é—´åºåˆ—é‡æ„å›¾ç»˜åˆ¶å¤±è´¥: {e}")
    
    # 3. ç»˜åˆ¶é¢å¤–çš„åˆ†æå›¾ï¼ˆå¯é€‰ï¼‰
    if save_dir:
        # ä¿å­˜æ¨¡å‹ç»“æœçš„è¯¦ç»†ä¿¡æ¯
        info_path = os.path.join(save_dir, f"{title_prefix}model_info.txt")
        with open(info_path, 'w', encoding='utf-8') as f:
            f.write(f"æ¨¡å‹ç»“æœåˆ†ææŠ¥å‘Š\n")
            f.write(f"=" * 50 + "\n\n")
            
            if predicted_prob_matrix is not None:
                f.write(f"æ¦‚ç‡çŸ©é˜µç»Ÿè®¡:\n")
                f.write(f"  - å½¢çŠ¶: {predicted_prob_matrix.shape}\n")
                f.write(f"  - æœ€å°å€¼: {np.min(predicted_prob_matrix):.4f}\n")
                f.write(f"  - æœ€å¤§å€¼: {np.max(predicted_prob_matrix):.4f}\n")
                f.write(f"  - å‡å€¼: {np.mean(predicted_prob_matrix):.4f}\n")
                f.write(f"  - æ ‡å‡†å·®: {np.std(predicted_prob_matrix):.4f}\n")
                f.write(f"  - ç¨€ç–åº¦: {np.mean(predicted_prob_matrix < 0.1):.3f}\n\n")
            
            if reconstructed_data is not None and original_data is not None:
                mse = np.mean((original_data - reconstructed_data) ** 2)
                mae = np.mean(np.abs(original_data - reconstructed_data))
                f.write(f"é‡æ„æ€§èƒ½:\n")
                f.write(f"  - MSE: {mse:.6f}\n")
                f.write(f"  - MAE: {mae:.6f}\n")
                f.write(f"  - æ˜¾ç¤ºçš„æ—¶é—´æ­¥æ•°: {min(max_timesteps, original_data.shape[0])}\n")
                if selected_nodes:
                    f.write(f"  - æ˜¾ç¤ºçš„èŠ‚ç‚¹: {selected_nodes}\n")
        
        print(f"ç»“æœåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {info_path}")


def plot_enhanced_time_series_reconstruction(original_data, reconstructed_data, 
                                           selected_node=None, max_timesteps=200,
                                           show_error_band=True, show_statistics=True,
                                           save_path=None, title_prefix="",
                                           comparison_mode="single", **kwargs):
    """
    Orchestrator for generating various types of time series reconstruction visualizations.
    All text output is in English.
    """
    # è®¾ç½®matplotlibåç«¯ï¼ˆåœ¨å¯¼å…¥pyplotä¹‹å‰ï¼‰
    import matplotlib
    matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
    import matplotlib.pyplot as plt
    plt.style.use('default')
    
    # å‚æ•°è®¾ç½®
    figsize = kwargs.get('figsize', (15, 8))
    line_width = kwargs.get('line_width', 2)
    alpha = kwargs.get('alpha', 0.8)
    error_alpha = kwargs.get('error_alpha', 0.3)
    
    # æ•°æ®é¢„å¤„ç†
    original_data, reconstructed_data, num_nodes, timesteps = _preprocess_time_series_data(
        original_data, reconstructed_data, max_timesteps
    )
    
    print(f"ğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆ:")
    print(f"   â€¢ æ—¶é—´æ­¥æ•°: {timesteps}")
    print(f"   â€¢ èŠ‚ç‚¹æ•°é‡: {num_nodes}")
    print(f"   â€¢ æ•°æ®å½¢çŠ¶: åŸå§‹{original_data.shape}, é‡æ„{reconstructed_data.shape}")
    
    # èŠ‚ç‚¹é€‰æ‹©é€»è¾‘
    if selected_node is None:
        selected_node = _auto_select_representative_node(original_data)
        print(f"   â€¢ è‡ªåŠ¨é€‰æ‹©èŠ‚ç‚¹: {selected_node + 1} (æœ€å…·ä»£è¡¨æ€§)")
    else:
        if selected_node >= num_nodes:
            selected_node = num_nodes - 1
            print(f"   âš ï¸  èŠ‚ç‚¹ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º: {selected_node + 1}")
    
    # æ ¹æ®å¯¹æ¯”æ¨¡å¼ç»˜åˆ¶
    if comparison_mode == "single":
        _plot_single_node_detailed(original_data, reconstructed_data, selected_node,
                                 timesteps, show_error_band, show_statistics,
                                 figsize, line_width, alpha, error_alpha, title_prefix)
    elif comparison_mode == "multi":
        _plot_multi_node_overview(original_data, reconstructed_data, 
                                max_nodes=min(6, num_nodes), timesteps=timesteps,
                                figsize=(16, 12), title_prefix=title_prefix)
    elif comparison_mode == "grid":
        _plot_grid_comparison(original_data, reconstructed_data,
                            selected_node, timesteps, title_prefix)
    
    # ä¿å­˜å›¾åƒ
    if save_path:
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            import os
            save_dir = os.path.dirname(save_path)
            if save_dir:  # å¦‚æœæœ‰ç›®å½•éƒ¨åˆ†
                os.makedirs(save_dir, exist_ok=True)
                print(f"   ğŸ“‚ ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨: {save_dir}")
            
            # è®¾ç½®matplotlibåç«¯
            import matplotlib
            matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
            
            # ä¿å­˜å›¾åƒï¼Œä½¿ç”¨æ›´å¤šå‚æ•°ç¡®ä¿æˆåŠŸ
            plt.savefig(save_path, dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none', 
                       format='png', transparent=False,
                       pad_inches=0.1)  # æ·»åŠ ä¸€äº›è¾¹è·
            
            print(f"   ğŸ’¾ å°è¯•ä¿å­˜åˆ°: {save_path}")
            
            # å¼ºåˆ¶åˆ·æ–°å¹¶ç­‰å¾…
            plt.draw()
            import time
            time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…ç¡®ä¿ä¿å­˜å®Œæˆ
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦ç¡®å®ä¿å­˜
            if os.path.exists(save_path):
                file_size = os.path.getsize(save_path)
                if file_size > 0:
                    print(f"   âœ… å›¾åƒå·²æˆåŠŸä¿å­˜åˆ°: {save_path}")
                    print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                else:
                    print(f"   âŒ æ–‡ä»¶å·²åˆ›å»ºä½†å¤§å°ä¸º0: {save_path}")
            else:
                print(f"   âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {save_path}")
                # å°è¯•ä½¿ç”¨ç»å¯¹è·¯å¾„ä¿å­˜
                abs_path = os.path.abspath(save_path)
                print(f"   ğŸ”„ å°è¯•ä½¿ç”¨ç»å¯¹è·¯å¾„: {abs_path}")
                plt.savefig(abs_path, dpi=300, bbox_inches='tight', 
                           facecolor='white', edgecolor='none', 
                           format='png', transparent=False)
                
        except Exception as save_error:
            print(f"   âŒ ä¿å­˜å›¾åƒæ—¶å‘ç”Ÿé”™è¯¯: {save_error}")
            import traceback
            traceback.print_exc()
            
            # å°è¯•å¤‡ç”¨ä¿å­˜æ–¹æ³•
            try:
                backup_path = save_path.replace('.png', '_backup.png')
                plt.savefig(backup_path, dpi=200)  # é™ä½DPIå†è¯•
                print(f"   ğŸ”„ å·²å°è¯•å¤‡ç”¨ä¿å­˜: {backup_path}")
            except Exception as backup_error:
                print(f"   âŒ å¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥: {backup_error}")
    
    plt.close('all')  # é‡Šæ”¾å†…å­˜è€Œä¸æ˜¯æ˜¾ç¤º
    
    # è¿”å›æˆåŠŸçŠ¶æ€
    if save_path:
        import os
        return os.path.exists(save_path) and os.path.getsize(save_path) > 0
    else:
        return True  # å¦‚æœæ²¡æœ‰æŒ‡å®šä¿å­˜è·¯å¾„ï¼Œè®¤ä¸ºæˆåŠŸ


def _preprocess_time_series_data(original_data, reconstructed_data, max_timesteps):
    """é¢„å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œç»Ÿä¸€æ ¼å¼"""
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if isinstance(original_data, torch.Tensor):
        original_data = original_data.detach().cpu().numpy()
    if isinstance(reconstructed_data, torch.Tensor):
        reconstructed_data = reconstructed_data.detach().cpu().numpy()
    
    # å¤„ç†ä¸åŒçš„è¾“å…¥æ ¼å¼
    original_data = _reshape_to_time_series(original_data)
    reconstructed_data = _reshape_to_time_series(reconstructed_data)
    
    # ç¡®ä¿ä¸¤ä¸ªæ•°æ®å½¢çŠ¶ä¸€è‡´
    if original_data.shape != reconstructed_data.shape:
        print(f"âš ï¸  æ•°æ®å½¢çŠ¶ä¸åŒ¹é…: åŸå§‹{original_data.shape} vs é‡æ„{reconstructed_data.shape}")
        # å°è¯•å¯¹é½
        min_timesteps = min(original_data.shape[0], reconstructed_data.shape[0])
        min_nodes = min(original_data.shape[1], reconstructed_data.shape[1])
        original_data = original_data[:min_timesteps, :min_nodes]
        reconstructed_data = reconstructed_data[:min_timesteps, :min_nodes]
    
    # é™åˆ¶æ—¶é—´æ­¥æ•°
    timesteps = min(original_data.shape[0], max_timesteps)
    original_data = original_data[:timesteps]
    reconstructed_data = reconstructed_data[:timesteps]
    
    num_nodes = original_data.shape[1]
    
    return original_data, reconstructed_data, num_nodes, timesteps


def _reshape_to_time_series(data):
    """å°†æ•°æ®é‡å¡‘ä¸º [timesteps, nodes] æ ¼å¼"""
    
    if data.ndim == 3:
        # [batch, timesteps, nodes] æˆ– [batch, nodes, timesteps]
        if data.shape[0] == 1:
            data = data[0]  # ç§»é™¤batchç»´åº¦
        else:
            data = data[0]  # å–ç¬¬ä¸€ä¸ªbatch
            print(f"   â„¹ï¸  æ£€æµ‹åˆ°å¤šä¸ªbatchï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªbatch")
    
    if data.ndim == 2:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬ç½®
        if data.shape[0] < data.shape[1]:
            # å¯èƒ½æ˜¯ [nodes, timesteps]ï¼Œè½¬ç½®ä¸º [timesteps, nodes]
            if data.shape[0] < 20:  # å¯å‘å¼åˆ¤æ–­ï¼šèŠ‚ç‚¹æ•°é€šå¸¸å°äº20
                data = data.T
                print(f"   ğŸ”„ æ•°æ®å·²è½¬ç½®ä¸º [timesteps, nodes] æ ¼å¼")
    
    return data


def _auto_select_representative_node(data):
    """è‡ªåŠ¨é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„èŠ‚ç‚¹"""
    
    if data.ndim == 1 or data.shape[1] == 1:
        return 0
    
    try:
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ä»£è¡¨æ€§æŒ‡æ ‡
        variances = np.var(data, axis=0)      # æ–¹å·®ï¼ˆä¿¡å·å¼ºåº¦ï¼‰
        ranges = np.ptp(data, axis=0)         # æå·®ï¼ˆåŠ¨æ€èŒƒå›´ï¼‰
        
        # é¿å…é€‰æ‹©å¸¸æ•°èŠ‚ç‚¹ï¼ˆæ–¹å·®ä¸º0ï¼‰
        valid_nodes = np.where(variances > 1e-10)[0]
        if len(valid_nodes) == 0:
            print(f"   âš ï¸ æ‰€æœ‰èŠ‚ç‚¹æ–¹å·®è¿‡å°ï¼Œä½¿ç”¨èŠ‚ç‚¹0")
            return 0
        
        # é¿å…NaNå’Œæ— ç©·å¤§å€¼
        variances = np.nan_to_num(variances, nan=0.0, posinf=0.0, neginf=0.0)
        ranges = np.nan_to_num(ranges, nan=0.0, posinf=0.0, neginf=0.0)
        
        # ç»¼åˆè¯„åˆ†ï¼šä¼˜å…ˆé€‰æ‹©æ–¹å·®å¤§ä¸”åŠ¨æ€èŒƒå›´å¤§çš„èŠ‚ç‚¹
        scores = variances + 0.5 * ranges
        valid_scores = scores[valid_nodes]
        
        if np.all(valid_scores == 0):
            print(f"   âš ï¸ æ‰€æœ‰èŠ‚ç‚¹è¯„åˆ†ä¸º0ï¼Œä½¿ç”¨èŠ‚ç‚¹0")
            return 0
            
        best_idx = np.argmax(valid_scores)
        best_node = valid_nodes[best_idx]
        
        print(f"   ğŸ¯ è‡ªåŠ¨é€‰æ‹©èŠ‚ç‚¹ {best_node}ï¼Œæ–¹å·®={variances[best_node]:.6f}")
        return best_node
        
    except Exception as e:
        print(f"   âŒ èŠ‚ç‚¹é€‰æ‹©å¤±è´¥: {e}")
        return 0


def _plot_single_node_detailed(original_data, reconstructed_data, selected_node,
                              timesteps, show_error_band, show_statistics,
                              figsize, line_width, alpha, error_alpha, title_prefix):
    """ç»˜åˆ¶å•èŠ‚ç‚¹è¯¦ç»†å¯¹æ¯”å›¾"""
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    time_axis = np.arange(timesteps)
    
    # æå–é€‰ä¸­èŠ‚ç‚¹çš„æ•°æ®
    orig_series = original_data[:, selected_node]
    recon_series = reconstructed_data[:, selected_node]
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ - æ·»åŠ å®‰å…¨æ£€æŸ¥
    try:
        mse = np.mean((orig_series - recon_series) ** 2)
        mae = np.mean(np.abs(orig_series - recon_series))
        rmse = np.sqrt(mse)
        
        # å®‰å…¨çš„ç›¸å…³æ€§è®¡ç®—
        if len(orig_series) > 1 and np.std(orig_series) > 1e-10 and np.std(recon_series) > 1e-10:
            correlation = np.corrcoef(orig_series, recon_series)[0, 1]
            # å¤„ç†NaNå€¼
            if np.isnan(correlation):
                correlation = 0.0
        else:
            correlation = 1.0 if np.allclose(orig_series, recon_series, atol=1e-6) else 0.0
    except Exception as e:
        print(f"   âš ï¸ ç»Ÿè®¡è®¡ç®—å‡ºé”™: {e}")
        mse = mae = rmse = correlation = 0.0
    
    # 1. ä¸»è¦å¯¹æ¯”å›¾ (å·¦ä¸Š)
    axes[0, 0].plot(time_axis, orig_series, label='Original Data', 
                   color='#1f77b4', linewidth=line_width, alpha=alpha)
    axes[0, 0].plot(time_axis, recon_series, label='Reconstructed Data', 
                   color='#ff7f0e', linewidth=line_width, alpha=alpha, linestyle='--')
    
    if show_error_band:
        axes[0, 0].fill_between(time_axis, orig_series, recon_series,
                               alpha=error_alpha, color='red', label='Reconstruction Error')
    
    axes[0, 0].set_title(f'Node {selected_node + 1} - Time Series Reconstruction Comparison')
    axes[0, 0].set_xlabel('Time Steps')
    axes[0, 0].set_ylabel('Values')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. è¯¯å·®åºåˆ—å›¾ (å³ä¸Š)
    error_series = orig_series - recon_series
    axes[0, 1].plot(time_axis, error_series, color='red', linewidth=1.5, alpha=0.8)
    axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)
    axes[0, 1].fill_between(time_axis, error_series, 0, alpha=0.3, color='red')
    axes[0, 1].set_title('Reconstruction Error Series')
    axes[0, 1].set_xlabel('Time Steps')
    axes[0, 1].set_ylabel('Error Values')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æ•£ç‚¹å›¾å¯¹æ¯” (å·¦ä¸‹)
    axes[1, 0].scatter(orig_series, recon_series, alpha=0.6, s=20, color='purple')
    
    # æ·»åŠ ç†æƒ³å¯¹è§’çº¿
    min_val = min(orig_series.min(), recon_series.min())
    max_val = max(orig_series.max(), recon_series.max())
    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Ideal Reconstruction')
    
    axes[1, 0].set_xlabel('Original Data')
    axes[1, 0].set_ylabel('Reconstructed Data')
    axes[1, 0].set_title(f'Scatter Plot Comparison (Correlation: {correlation:.3f})')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. ç»Ÿè®¡ä¿¡æ¯å±•ç¤º (å³ä¸‹)
    if show_statistics:
        axes[1, 1].axis('off')
        
        # æ›´å¤šç»Ÿè®¡æŒ‡æ ‡
        percentile_errors = np.percentile(np.abs(error_series), [50, 90, 95, 99])
        signal_power = np.mean(orig_series ** 2)
        noise_power = np.mean(error_series ** 2)
        snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
        
        stats_text = f"""ğŸ“Š Reconstruction Performance Statistics
        
ğŸ¯ Basic Metrics:
   â€¢ MSE: {mse:.6f}
   â€¢ RMSE: {rmse:.6f}  
   â€¢ MAE: {mae:.6f}
   â€¢ Correlation: {correlation:.4f}

ğŸ“ˆ Error Distribution:
   â€¢ Median Error: {percentile_errors[0]:.6f}
   â€¢ 90th Percentile: {percentile_errors[1]:.6f}
   â€¢ 95th Percentile: {percentile_errors[2]:.6f}
   â€¢ 99th Percentile: {percentile_errors[3]:.6f}

ğŸ”Š Signal-to-Noise Ratio:
   â€¢ SNR: {snr:.2f} dB
   
ğŸ“ Data Range:
   â€¢ Original Range: [{orig_series.min():.3f}, {orig_series.max():.3f}]
   â€¢ Reconstructed Range: [{recon_series.min():.3f}, {recon_series.max():.3f}]
   
â±ï¸ Display Info:
   â€¢ Node Index: {selected_node + 1}
   â€¢ Time Steps: {timesteps}
        """
        
        axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes,
                        fontsize=10, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    
    plt.suptitle(f'{title_prefix}Node {selected_node + 1} Time Series Reconstruction Analysis', fontsize=16, fontweight='bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])


def _plot_multi_node_overview(original_data, reconstructed_data, max_nodes, timesteps, figsize, title_prefix):
    """ç»˜åˆ¶å¤šèŠ‚ç‚¹æ¦‚è§ˆå¯¹æ¯”å›¾"""
    
    num_nodes = original_data.shape[1]
    nodes_to_show = min(max_nodes, num_nodes)
    
    # é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„èŠ‚ç‚¹
    variances = np.var(original_data, axis=0)
    top_nodes = np.argsort(variances)[-nodes_to_show:]
    
    fig, axes = plt.subplots(nodes_to_show, 1, figsize=figsize, sharex=True)
    if nodes_to_show == 1:
        axes = [axes]
    
    time_axis = np.arange(timesteps)
    
    for i, node_idx in enumerate(top_nodes):
        orig_series = original_data[:, node_idx]
        recon_series = reconstructed_data[:, node_idx]
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡ - å®‰å…¨è®¡ç®—
        try:
            mse = np.mean((orig_series - recon_series) ** 2)
            if len(orig_series) > 1 and np.std(orig_series) > 1e-10 and np.std(recon_series) > 1e-10:
                correlation = np.corrcoef(orig_series, recon_series)[0, 1]
                if np.isnan(correlation):
                    correlation = 0.0
            else:
                correlation = 1.0 if np.allclose(orig_series, recon_series, atol=1e-6) else 0.0
        except Exception as e:
            print(f"   âš ï¸ èŠ‚ç‚¹{node_idx}ç»Ÿè®¡è®¡ç®—å‡ºé”™: {e}")
            mse = correlation = 0.0
        
        axes[i].plot(time_axis, orig_series, label='Original', color='blue', linewidth=1.5, alpha=0.8)
        axes[i].plot(time_axis, recon_series, label='Reconstructed', color='red', linewidth=1.5, alpha=0.8, linestyle='--')
        
        axes[i].set_title(f'Node {node_idx + 1} - MSE: {mse:.4f}, Correlation: {correlation:.3f}')
        axes[i].set_ylabel('Values')
        axes[i].legend(loc='upper right')
        axes[i].grid(True, alpha=0.3)
    
    axes[-1].set_xlabel('Time Steps')
    plt.suptitle(f'{title_prefix}Multi-Node Time Series Reconstruction Overview (First {timesteps} Steps)', fontsize=16, fontweight='bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])


def _plot_grid_comparison(original_data, reconstructed_data, selected_node, timesteps, title_prefix):
    """ç»˜åˆ¶ç½‘æ ¼å¸ƒå±€å¯¹æ¯”å›¾"""
    
    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    time_axis = np.arange(timesteps)
    orig_series = original_data[:, selected_node]
    recon_series = reconstructed_data[:, selected_node]
    
    # ä¸»å›¾ - å æ®å·¦ä¾§ä¸¤åˆ—
    ax_main = fig.add_subplot(gs[:, :2])
    ax_main.plot(time_axis, orig_series, label='Original Data', color='blue', linewidth=2, alpha=0.8)
    ax_main.plot(time_axis, recon_series, label='Reconstructed Data', color='red', linewidth=2, alpha=0.8, linestyle='--')
    ax_main.fill_between(time_axis, orig_series, recon_series, alpha=0.2, color='orange')
    ax_main.set_title(f'Node {selected_node + 1} - Complete Time Series Comparison ({timesteps} Steps)')
    ax_main.set_xlabel('Time Steps')
    ax_main.set_ylabel('Values')
    ax_main.legend()
    ax_main.grid(True, alpha=0.3)
    
    # å±€éƒ¨æ”¾å¤§å›¾1 - å‰1/3
    ax_zoom1 = fig.add_subplot(gs[0, 2])
    start_idx, end_idx = 0, timesteps // 3
    _plot_zoomed_section(ax_zoom1, time_axis[start_idx:end_idx], 
                        orig_series[start_idx:end_idx], recon_series[start_idx:end_idx],
                        f'First 1/3 Section (Steps {start_idx+1}-{end_idx})')
    
    # å±€éƒ¨æ”¾å¤§å›¾2 - ä¸­é—´1/3  
    ax_zoom2 = fig.add_subplot(gs[1, 2])
    start_idx, end_idx = timesteps // 3, 2 * timesteps // 3
    _plot_zoomed_section(ax_zoom2, time_axis[start_idx:end_idx],
                        orig_series[start_idx:end_idx], recon_series[start_idx:end_idx],
                        f'Middle 1/3 Section (Steps {start_idx+1}-{end_idx})')
    
    # å±€éƒ¨æ”¾å¤§å›¾3 - å1/3
    ax_zoom3 = fig.add_subplot(gs[2, 2])
    start_idx, end_idx = 2 * timesteps // 3, timesteps
    _plot_zoomed_section(ax_zoom3, time_axis[start_idx:end_idx],
                        orig_series[start_idx:end_idx], recon_series[start_idx:end_idx],
                        f'Last 1/3 Section (Steps {start_idx+1}-{end_idx})')
    
    plt.suptitle(f'{title_prefix}Node {selected_node + 1} Grid-Based Detailed Comparison Analysis', fontsize=16, fontweight='bold')


def _plot_zoomed_section(ax, time_axis, orig_data, recon_data, title):
    """ç»˜åˆ¶å±€éƒ¨æ”¾å¤§åŒºåŸŸ"""
    ax.plot(time_axis, orig_data, color='blue', linewidth=1.5, alpha=0.8, label='Original')
    ax.plot(time_axis, recon_data, color='red', linewidth=1.5, alpha=0.8, linestyle='--', label='Reconstructed')
    ax.fill_between(time_axis, orig_data, recon_data, alpha=0.3, color='orange')
    
    # è®¡ç®—è¯¥åŒºé—´çš„MSE
    mse = np.mean((orig_data - recon_data) ** 2)
    ax.set_title(f'{title}\nMSE: {mse:.6f}', fontsize=10)
    ax.grid(True, alpha=0.3)
    ax.legend(fontsize=8)


class PFNOVisualizationSuite:
    """PFNOå¯è§†åŒ–å¥—ä»¶ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½"""
    
    def __init__(self, results_dir, config=None):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å¥—ä»¶
        
        Args:
            results_dir: ç»“æœä¿å­˜ç›®å½•
            config: å¯è§†åŒ–é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        self.results_dir = results_dir
        self.config = config or {}
        
        # é»˜è®¤å¯è§†åŒ–é…ç½®
        self.viz_config = {
            'training_curves': {
                'figsize': (16, 12),
                'smooth_window': 'auto',  # auto, int, or None
                'show_components': True,
                'save_format': 'png'
            },
            'matrix_comparison': {
                'figsize': (14, 10),
                'cmap': 'Blues',
                'threshold': 0.5,
                'show_colorbar': True,
                'title_prefix': "PFNO Causal Discovery Results - "
            },
            'time_series': {
                'figsize_single': (16, 10),
                'figsize_multi': (16, 12),
                'figsize_grid': (20, 14),
                'max_timesteps': 200,
                'selected_nodes': 'auto',  # 'auto', list of indices, or int
                'show_error_band': True,
                'show_statistics': True,
                'title_prefix': "PFNOé‡æ„åˆ†æ - "
            }
        }
        
        # æ›´æ–°ç”¨æˆ·é…ç½®
        if 'visualization' in self.config:
            self._update_config(self.config['visualization'])
        
        print(f"ğŸ¨ PFNOå¯è§†åŒ–å¥—ä»¶å·²åˆå§‹åŒ–")
        print(f"   - ç»“æœç›®å½•: {self.results_dir}")
    
    def _update_config(self, user_config):
        """æ›´æ–°å¯è§†åŒ–é…ç½®"""
        for category, settings in user_config.items():
            if category in self.viz_config:
                self.viz_config[category].update(settings)
    
    def plot_training_curves_enhanced(self, train_losses, val_losses, loss_components):
        """ç»˜åˆ¶å¢å¼ºçš„è®­ç»ƒæŸå¤±æ›²çº¿"""
        try:
            print(f"   ğŸ“ˆ æ­£åœ¨ç”Ÿæˆè®­ç»ƒæ›²çº¿...")
            print(f"   ğŸ“Š è®­ç»ƒè½®æ•°: {len(train_losses)}")
            
            # å‡†å¤‡è®­ç»ƒå†å²æ•°æ®
            training_history = {
                'train_losses': train_losses,
                'val_losses': val_losses,
                'loss_components': loss_components
            }
            
            # æ·»åŠ å¹³æ»‘å¤„ç†
            smooth_config = self.viz_config['training_curves']
            if len(train_losses) >= 5 and smooth_config['smooth_window'] != 'none':
                try:
                    import pandas as pd
                    
                    # è‡ªé€‚åº”çª—å£å¤§å°
                    if smooth_config['smooth_window'] == 'auto':
                        window = max(3, min(9, len(train_losses) // 4))
                    else:
                        window = smooth_config['smooth_window']
                    
                    print(f"   ğŸ”§ ä½¿ç”¨ç§»åŠ¨å¹³å‡çª—å£: {window}")
                    
                    # è®¡ç®—ç§»åŠ¨å¹³å‡
                    train_smooth = pd.Series(train_losses).rolling(window=window, center=True).mean()
                    val_smooth = pd.Series(val_losses).rolling(window=window, center=True).mean()
                    
                    # å¤„ç†è¾¹ç•ŒNaNå€¼
                    training_history['train_losses_smooth'] = train_smooth.bfill().ffill().tolist()
                    training_history['val_losses_smooth'] = val_smooth.bfill().ffill().tolist()
                    
                    print(f"   âœ¨ å·²åº”ç”¨å¹³æ»‘å¤„ç†")
                except ImportError:
                    print(f"   âš ï¸ pandasä¸å¯ç”¨ï¼Œè·³è¿‡å¹³æ»‘å¤„ç†")
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            save_path = f"{self.results_dir}/training_curves_final.png"
            plot_training_curves(
                training_history, 
                save_path=save_path, 
                figsize=smooth_config['figsize']
            )
            
            print(f"   âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
            return True
            
        except Exception as e:
            print(f"   âŒ ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def plot_matrix_comparison_enhanced(self, predicted_matrix, true_matrix):
        """ç»˜åˆ¶å¢å¼ºçš„çŸ©é˜µå¯¹æ¯”å›¾"""
        if true_matrix is None:
            print("   âš ï¸ æ²¡æœ‰çœŸå®é‚»æ¥çŸ©é˜µï¼Œè·³è¿‡å¯¹æ¯”å›¾ç»˜åˆ¶")
            return False
        
        try:
            print(f"   ğŸ“Š æ­£åœ¨ç”Ÿæˆé‚»æ¥çŸ©é˜µå¯¹æ¯”å›¾...")
            
            matrix_config = self.viz_config['matrix_comparison']
            print(f"   ğŸ¯ ä½¿ç”¨ç½®ä¿¡åº¦é˜ˆå€¼: {matrix_config['threshold']:.3f}")
            print(f"   ğŸ“ çŸ©é˜µå°ºå¯¸: {predicted_matrix.shape}")
            
            # ä½¿ç”¨ç°æœ‰çš„çŸ©é˜µå¯¹æ¯”å¯è§†åŒ–å‡½æ•°
            save_path = f"{self.results_dir}/adjacency_matrix_comparison.png"
            plot_probability_matrix_comparison(
                predicted_prob_matrix=predicted_matrix,
                true_causal_matrix=true_matrix,
                save_path=save_path,
                title_prefix=matrix_config['title_prefix'],
                threshold=matrix_config['threshold'],
                cmap=matrix_config['cmap']
            )
            
            print(f"   âœ… é‚»æ¥çŸ©é˜µå¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
            return True
            
        except Exception as e:
            print(f"   âŒ ç»˜åˆ¶çŸ©é˜µå¯¹æ¯”å›¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_time_series_visualizations(self, model, test_data, raw_data):
        """
        Generates a comprehensive set of long-sequence time series reconstruction visualizations.
        This function creates detailed, overview, and grid plots for up to 200 timesteps.
        """
        try:
            print(f"   ğŸ“ˆ Generating long-sequence time series reconstruction visualizations...")
            import os
            os.makedirs(self.results_dir, exist_ok=True)
            print(f"   ğŸ“‚ Results will be saved in: {self.results_dir}")

            # --- 1. Infer model's processing structure (chunk size and number of nodes) ---
            structure_info = self._infer_model_structure(model, test_data)
            if not structure_info:
                print("   âŒ Could not infer model structure. Visualization terminated.")
                return False
            timesteps_per_chunk, num_nodes = structure_info
            
            # --- 2. Prepare original data for 200 timesteps ---
            print("   ğŸ“– Step 2: Preparing original data for 200 timesteps...")
            full_original_data = self._prepare_original_data(raw_data, test_data)
            if full_original_data is None:
                print(f"   âŒ Could not load original data. Visualization terminated.")
                return False
            
            max_timesteps_to_show = 200
            if full_original_data.shape[0] < max_timesteps_to_show:
                print(f"   âš ï¸ Original data has fewer than 200 timesteps ({full_original_data.shape[0]} available). Using all available data.")
                max_timesteps_to_show = full_original_data.shape[0]

            original_ts_long = full_original_data[:max_timesteps_to_show, :num_nodes]
            print(f"   âœ… Prepared original data for comparison, shape: {original_ts_long.shape}")

            # --- 3. Iteratively reconstruct the long sequence ---
            print(f"   ğŸš€ Step 3: Reconstructing sequence of length {max_timesteps_to_show}...")
            reconstructed_ts_long = self._reconstruct_long_sequence(
                model, original_ts_long, timesteps_per_chunk, num_nodes
            )
            if reconstructed_ts_long is None:
                print("   âŒ Long sequence reconstruction failed. Visualization terminated.")
                return False
            print(f"   âœ… Long sequence reconstruction complete, shape: {reconstructed_ts_long.shape}")

            # --- 4. Generate and save all visualizations ---
            print("   ğŸ¨ Step 4: Plotting and saving all visualization types...")
            ts_config = self.viz_config['time_series']
            selected_nodes = self._select_nodes_for_visualization(original_ts_long, ts_config['selected_nodes'])
            success_count = 0
            
            # Plot 1: Detailed single-node analysis
            try:
                save_path_detailed = os.path.join(self.results_dir, "time_series_reconstruction_detailed.png")
                plot_enhanced_time_series_reconstruction(
                    original_data=original_ts_long,
                    reconstructed_data=reconstructed_ts_long,
                    selected_node=selected_nodes[0] if selected_nodes else None,
                    max_timesteps=max_timesteps_to_show,
                    comparison_mode="single",
                    save_path=save_path_detailed,
                    title_prefix="Detailed Reconstruction Analysis - "
                )
                if os.path.exists(save_path_detailed): success_count += 1
            except Exception as e:
                print(f"   âŒ Failed to generate detailed plot: {e}")

            # Plot 2: Multi-node overview
            try:
                save_path_overview = os.path.join(self.results_dir, "time_series_reconstruction_overview.png")
                plot_enhanced_time_series_reconstruction(
                    original_data=original_ts_long,
                    reconstructed_data=reconstructed_ts_long,
                    max_timesteps=max_timesteps_to_show,
                    comparison_mode="multi",
                    save_path=save_path_overview,
                    title_prefix="Multi-Node Reconstruction Overview - "
                )
                if os.path.exists(save_path_overview): success_count += 1
            except Exception as e:
                print(f"   âŒ Failed to generate overview plot: {e}")

            # Plot 3: Grid comparison
            try:
                save_path_grid = os.path.join(self.results_dir, "time_series_reconstruction_grid.png")
                plot_enhanced_time_series_reconstruction(
                    original_data=original_ts_long,
                    reconstructed_data=reconstructed_ts_long,
                    selected_node=selected_nodes[0] if selected_nodes else 0,
                    max_timesteps=max_timesteps_to_show,
                    comparison_mode="grid",
                    save_path=save_path_grid,
                    title_prefix="Grid-Based Reconstruction Analysis - "
                )
                if os.path.exists(save_path_grid): success_count += 1
            except Exception as e:
                print(f"   âŒ Failed to generate grid plot: {e}")

            print(f"   âœ… Visualization generation finished. {success_count}/3 plots saved successfully.")

            # --- 5. Save statistical data ---
            print("   ğŸ’¾ Step 5: Saving detailed statistics...")
            try:
                self._save_reconstruction_statistics(original_ts_long, reconstructed_ts_long)
            except Exception as e:
                print(f"   âŒ Failed to save reconstruction statistics: {e}")

            return success_count > 0

        except Exception as e:
            print(f"   âŒ Failed to generate time series visualizations: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _infer_model_structure(self, model, test_data):
        """é€šè¿‡å•æ¬¡è¿è¡Œæ¨æ–­æ¨¡å‹çš„è¾“å…¥ç»“æ„ï¼ˆæ—¶é—´æ­¥é•¿å’ŒèŠ‚ç‚¹æ•°ï¼‰ã€‚"""
        try:
            print("   ğŸ§© æ­£åœ¨æ¨æ–­æ¨¡å‹ç»“æ„...")
            if not hasattr(test_data, 'shape') or len(test_data.shape) < 2:
                 raise ValueError(f"æµ‹è¯•æ•°æ®å½¢çŠ¶æ— æ•ˆ: {test_data.shape if hasattr(test_data, 'shape') else 'N/A'}")

            num_features = test_data.shape[1]
            
            # ä¼˜å…ˆä»æ¨¡å‹å±æ€§è·å–èŠ‚ç‚¹æ•°
            num_nodes = getattr(model, 'num_nodes', None)
            if num_nodes is None:
                # å†æ¬¡å°è¯•ä»é…ç½®ä¸­è·å–
                num_nodes = self.config.get('num_nodes', None)
                if num_nodes is None:
                    # æœ€åï¼Œä½œä¸ºå¤‡é€‰ï¼Œå‡å®šä¸º10
                    num_nodes = 10
                    print(f"   âš ï¸ æ— æ³•ä»æ¨¡å‹æˆ–é…ç½®ä¸­è·å–èŠ‚ç‚¹æ•°ï¼Œå‡è®¾ä¸º: {num_nodes}")

            if num_features % num_nodes != 0:
                raise ValueError(f"ç‰¹å¾æ•°({num_features})ä¸èƒ½è¢«èŠ‚ç‚¹æ•°({num_nodes})æ•´é™¤")
            
            timesteps_per_chunk = num_features // num_nodes
            
            # å°†ç»“æ„ä¿å­˜åœ¨å®ä¾‹ä¸­ï¼Œæ–¹ä¾¿å¤ç”¨
            self.ts_structure = {'timesteps': timesteps_per_chunk, 'nodes': num_nodes}
            
            print(f"   âœ… æ¨¡å‹ç»“æ„æ¨æ–­æˆåŠŸ: æ¯ä¸ªåˆ‡ç‰‡åŒ…å« {timesteps_per_chunk} æ—¶é—´æ­¥ Ã— {num_nodes} èŠ‚ç‚¹ã€‚")
            return timesteps_per_chunk, num_nodes
        except Exception as e:
            print(f"   âŒ æ¨æ–­æ¨¡å‹ç»“æ„å¤±è´¥: {e}")
            return None

    def _reconstruct_long_sequence(self, model, original_sequence, timesteps_per_chunk, num_nodes):
        """ç”¨æ»‘åŠ¨çª—å£çš„æ–¹å¼é‡æ„ä¸€ä¸ªé•¿åºåˆ—"""
        model.eval()
        device = next(model.parameters()).device
        print(f"\nğŸ”„ é•¿åºåˆ—å°†è¢«åˆ‡åˆ†ä¸º {len(original_sequence) // timesteps_per_chunk} ä¸ªå—è¿›è¡Œå¤„ç†...")
        reconstructed_chunks = []
        num_total_timesteps = original_sequence.shape[0]
        
        # ç¡®ä¿æ€»æ—¶é—´æ­¥æ˜¯åˆ‡ç‰‡é•¿åº¦çš„æ•´æ•°å€
        if num_total_timesteps % timesteps_per_chunk != 0:
            new_len = (num_total_timesteps // timesteps_per_chunk) * timesteps_per_chunk
            print(f"   âš ï¸ æ€»æ—¶é—´æ­¥é•¿({num_total_timesteps})ä¸æ˜¯åˆ‡ç‰‡é•¿åº¦({timesteps_per_chunk})çš„æ•´æ•°å€ã€‚å°†æˆªæ–­ä¸º {new_len}ã€‚")
            original_sequence = original_sequence[:new_len, :]
            num_total_timesteps = new_len
        
        num_chunks = num_total_timesteps // timesteps_per_chunk
        print(f"   ğŸ”„ é•¿åºåˆ—å°†è¢«åˆ‡åˆ†ä¸º {num_chunks} ä¸ªå—è¿›è¡Œå¤„ç†...")
        
        with torch.no_grad():
            model.eval()
            device = next(model.parameters()).device
            
            for i in range(num_chunks):
                start = i * timesteps_per_chunk
                end = start + timesteps_per_chunk
                chunk_original_np = original_sequence[start:end, :]
                
                # Reshape to model input format [1, features]
                model_input_flat = torch.tensor(chunk_original_np.flatten(), dtype=torch.float32).unsqueeze(0)
                model_input_flat = model_input_flat.to(device)
                
                # Run model - æ˜¾å¼ä¼ é€’ only_prediction=True
                result = model(model_input_flat, only_prediction=True)

                # Extract prediction from dict or direct tensor
                prediction_output = None
                if isinstance(result, dict):
                    key_found = False
                    # ä¿®æ­£ï¼šåªå¯»æ‰¾ 'prediction' æˆ– 'prediction_mean'
                    if 'prediction' in result:
                        prediction_output = result['prediction']
                        key_found = True
                    elif 'prediction_mean' in result:
                        prediction_output = result['prediction_mean']
                        key_found = True

                    if not key_found:
                        print("   âŒ åœ¨æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„é¢„æµ‹é”®ã€‚")
                        return None
                else:
                    prediction_output = result
                    
                # Process prediction samples: e.g., [1, 50, 400] -> [1, 50]
                if prediction_output.ndim == 3:
                    reconstructed_flat = prediction_output.mean(dim=-1)
                else:
                    reconstructed_flat = prediction_output
                
                # Reshape back to [timesteps, nodes]
                reconstructed_chunk_np = reconstructed_flat.detach().cpu().numpy().reshape(timesteps_per_chunk, num_nodes)
                reconstructed_chunks.append(reconstructed_chunk_np)
        
        print("   âœ… æ‰€æœ‰å—å¤„ç†å®Œæ¯•ã€‚")
        return np.vstack(reconstructed_chunks)
    
    def _prepare_reconstruction_data(self, model, test_data, raw_data):
        """å‡†å¤‡é‡æ„æ•°æ®ï¼Œå¹¶æå–æ—¶é—´åºåˆ—çš„ç»“æ„ä¿¡æ¯"""
        try:
            print(f"   ğŸ”„ å‡†å¤‡é‡æ„æ•°æ®...")
            
            import torch
            import numpy as np
            
            with torch.no_grad():
                model.eval()
                device = next(model.parameters()).device
                test_data_tensor = test_data.to(device) if isinstance(test_data, torch.Tensor) else torch.tensor(test_data, device=device).float()
                
                print(f"   ğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {test_data_tensor.shape}")
                
                result = model(test_data_tensor)
                
                prediction_key = None
                if isinstance(result, dict):
                    for key in ['prediction_samples', 'predictions', 'reconstructed', 'output', 'x_pred']:
                        if key in result:
                            prediction_key = key
                            break
                    if not prediction_key:
                        raise ValueError("æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­æœªæ‰¾åˆ°é¢„æµ‹æ•°æ®")
                    prediction_output = result[prediction_key]
                else:
                    prediction_output = result

                print(f"   ğŸ“Š é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {prediction_output.shape}")

                # æå–é‡æ„æ•°æ®
                if prediction_output.ndim == 3:  # [batch, features, samples]
                    reconstructed_data = prediction_output.mean(dim=-1)
                elif prediction_output.ndim <= 2: # [batch, features] or [features]
                    reconstructed_data = prediction_output
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„é¢„æµ‹è¾“å‡ºç»´åº¦: {prediction_output.ndim}D")
                
                # ç¡®ä¿æ˜¯2D [batch, features]
                if reconstructed_data.ndim == 1:
                    reconstructed_data = reconstructed_data.unsqueeze(0)
                
                print(f"   ğŸ”„ é‡æ„æ•°æ®å½¢çŠ¶ (å¤„ç†å): {reconstructed_data.shape}")

                # æå–æ—¶é—´åºåˆ—ç»“æ„
                # å°è¯•ä»æ¨¡å‹å±æ€§è·å–
                num_nodes = getattr(model, 'num_nodes', None)
                if num_nodes is None:
                    # å¦‚æœæ¨¡å‹æ²¡æœ‰èŠ‚ç‚¹ä¿¡æ¯ï¼Œä»åŸå§‹æ•°æ®æ¨æ–­
                    if raw_data is not None and hasattr(raw_data, 'shape') and raw_data.ndim == 2:
                        num_nodes = raw_data.shape[1]
                    else: # æœ€åå°è¯•ä»é…ç½®ä¸­è·å–
                        num_nodes = self.config.get('num_nodes', 10) # é»˜è®¤ä¸º10
                
                features = reconstructed_data.shape[1]
                if features % num_nodes != 0:
                     print(f"   âš ï¸ ç‰¹å¾æ•°({features})ä¸èƒ½è¢«èŠ‚ç‚¹æ•°({num_nodes})æ•´é™¤ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®å½¢çŠ¶")
                     if raw_data is not None and hasattr(raw_data, 'shape') and raw_data.ndim == 2:
                         num_nodes = raw_data.shape[1]
                         timesteps = raw_data.shape[0]
                     else:
                        raise ValueError("æ— æ³•ç¡®å®šæ—¶é—´åºåˆ—ç»“æ„")
                else:
                    timesteps = features // num_nodes

                print(f"   ğŸ§© æ¨æ–­å‡ºçš„æ—¶é—´åºåˆ—ç»“æ„: {timesteps} æ—¶é—´æ­¥ Ã— {num_nodes} èŠ‚ç‚¹")

                # ä¿å­˜ç»“æ„ä¿¡æ¯ä»¥ä¾›åç»­æ­¥éª¤ä½¿ç”¨
                self.ts_structure = {'timesteps': timesteps, 'nodes': num_nodes}
                
                reconstructed_data_np = reconstructed_data.detach().cpu().numpy()
                return reconstructed_data_np

        except Exception as e:
            print(f"   âŒ å‡†å¤‡é‡æ„æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _prepare_original_data(self, raw_data, test_data):
        """å‡†å¤‡åŸå§‹æ•°æ® - ä¸“é—¨é€‚é…CSVæ ¼å¼æ•°æ®ï¼ˆè¡Œä¸ºæ ·æœ¬æ•°ï¼Œåˆ—ä¸ºèŠ‚ç‚¹ç§ç±»ï¼‰"""
        # æ–¹æ¡ˆ1: ç›´æ¥ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®
        csv_path = "data/generated_time_series2.csv"
        if Path(csv_path).exists():
            try:
                import pandas as pd
                print(f"   ğŸ“ ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®: {csv_path}")
                df = pd.read_csv(csv_path, header=None)
                original_data = df.values
                print(f"   âœ… æˆåŠŸåŠ è½½CSVæ•°æ®: {original_data.shape}")
                return original_data
            except Exception as e:
                print(f"   âŒ ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®å¤±è´¥: {e}")

        # æ–¹æ¡ˆ2: ä½¿ç”¨ä¼ å…¥çš„raw_data
        if raw_data is not None and hasattr(raw_data, 'shape'):
            try:
                original_data = raw_data.detach().cpu().numpy() if isinstance(raw_data, torch.Tensor) else np.array(raw_data)
                print(f"   ğŸ“Š ä½¿ç”¨ä¼ å…¥çš„åŸå§‹æ•°æ®: {original_data.shape}")
                return original_data
            except Exception as e:
                print(f"   âš ï¸ å¤„ç†ä¼ å…¥çš„åŸå§‹æ•°æ®å¤±è´¥: {e}")

        # æ–¹æ¡ˆ3: ä»test_dataæ¢å¤
        if test_data is not None and hasattr(test_data, 'shape'):
            try:
                original_data = test_data.detach().cpu().numpy() if isinstance(test_data, torch.Tensor) else np.array(test_data)
                print(f"   ğŸ“Š ä½¿ç”¨æµ‹è¯•æ•°æ®ä½œä¸ºåŸå§‹æ•°æ®: {original_data.shape}")
                return original_data
            except Exception as e:
                print(f"   âš ï¸ å¤„ç†æµ‹è¯•æ•°æ®å¤±è´¥: {e}")

        print(f"   âŒ æ— æ³•è·å–åŸå§‹æ•°æ®")
        return None
    
    def _align_data_shapes(self, original_data, reconstructed_data):
        """åˆ©ç”¨å·²æ¨æ–­çš„ç»“æ„ä¿¡æ¯ï¼Œå¯¹é½åŸå§‹å’Œé‡æ„æ•°æ®å½¢çŠ¶"""
        try:
            print(f"   ğŸ”„ å¼€å§‹æ•°æ®å½¢çŠ¶å¯¹é½...")
            print(f"   ğŸ“ è¾“å…¥å½¢çŠ¶: åŸå§‹{original_data.shape}, é‡æ„{reconstructed_data.shape}")

            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ç»“æ„ä¿¡æ¯
            if not hasattr(self, 'ts_structure') or not self.ts_structure:
                raise ValueError("æ—¶é—´åºåˆ—ç»“æ„ä¿¡æ¯ (ts_structure) ä¸å¯ç”¨")
                
            timesteps = self.ts_structure['timesteps']
            nodes = self.ts_structure['nodes']
            print(f"   ğŸ§© ä½¿ç”¨ç»“æ„ä¿¡æ¯: {timesteps} æ—¶é—´æ­¥ Ã— {nodes} èŠ‚ç‚¹")

            # é‡å¡‘å‡½æ•°
            def reshape_data(data, name):
                if data.ndim == 2 and data.shape[0] == 1: # [1, features]
                    data = data.flatten()
                
                if data.ndim == 1:
                    expected_size = timesteps * nodes
                    if len(data) == expected_size:
                        reshaped = data.reshape(timesteps, nodes)
                        print(f"   âœ… å·²å°† {name} ä»1Dé‡å¡‘ä¸º {reshaped.shape}")
                        return reshaped
                    else:
                        raise ValueError(f"{name} æ•°æ®é•¿åº¦({len(data)})ä¸æœŸæœ›å¤§å°({expected_size})ä¸åŒ¹é…")
                elif data.ndim == 2:
                    return data # å·²ç»æ˜¯2Dï¼Œç›´æ¥è¿”å›
                else:
                    raise ValueError(f"{name} æ•°æ®ç»´åº¦({data.ndim}D)å¼‚å¸¸")

            original_data = reshape_data(original_data, "åŸå§‹æ•°æ®")
            reconstructed_data = reshape_data(reconstructed_data, "é‡æ„æ•°æ®")

            # æˆªå–åˆ°ç›¸åŒçš„æœ€å°é•¿åº¦
            min_len = min(original_data.shape[0], reconstructed_data.shape[0])
            original_data = original_data[:min_len, :]
            reconstructed_data = reconstructed_data[:min_len, :]
            
            print(f"   âœ… æ•°æ®å½¢çŠ¶å¯¹é½å®Œæˆ: {original_data.shape}")
            return original_data, reconstructed_data

        except Exception as e:
            print(f"   âŒ æ•°æ®å¯¹é½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def _select_nodes_for_visualization(self, data, selection_config):
        """æ™ºèƒ½é€‰æ‹©è¦å¯è§†åŒ–çš„èŠ‚ç‚¹"""
        num_nodes = data.shape[1]
        
        if selection_config == 'auto':
            # è‡ªåŠ¨é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„èŠ‚ç‚¹
            variances = np.var(data, axis=0)
            selected_nodes = [np.argmax(variances)]  # é€‰æ‹©æ–¹å·®æœ€å¤§çš„èŠ‚ç‚¹
            if num_nodes > 1:
                sorted_indices = np.argsort(variances)[::-1]
                selected_nodes = sorted_indices[:min(3, num_nodes)].tolist()
        elif isinstance(selection_config, list):
            selected_nodes = [i for i in selection_config if 0 <= i < num_nodes]
        elif isinstance(selection_config, int):
            selected_nodes = [selection_config] if 0 <= selection_config < num_nodes else [0]
        else:
            selected_nodes = [0]  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
        
        return selected_nodes
    
    def _save_reconstruction_statistics(self, original_data, reconstructed_data):
        """ä¿å­˜é‡æ„æ€§èƒ½ç»Ÿè®¡ - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«æ¯ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†æŒ‡æ ‡"""
        try:
            print(f"   ğŸ“ˆ ä¿å­˜é‡æ„æ€§èƒ½ç»Ÿè®¡...")
            
            # ä¿®æ­£ï¼šåœ¨è®¡ç®—å‰ç¡®ä¿åŸå§‹æ•°æ®å’Œé‡æ„æ•°æ®çš„é•¿åº¦ä¸€è‡´
            # è¿™æ˜¯ä¸ºäº†é˜²æ­¢å› æ»‘åŠ¨çª—å£å¤„ç†è€Œå¯¼è‡´çš„æœ«ç«¯æ•°æ®ä¸åŒ¹é…é—®é¢˜
            min_len = min(len(original_data), len(reconstructed_data))
            original_data = original_data[:min_len]
            reconstructed_data = reconstructed_data[:min_len]
            
            # è®¡ç®—æ•´ä½“ç»Ÿè®¡æŒ‡æ ‡
            mse = np.mean((original_data - reconstructed_data) ** 2)
            mae = np.mean(np.abs(original_data - reconstructed_data))
            rmse = np.sqrt(mse)
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            correlation_matrix = np.corrcoef(original_data.flatten(), reconstructed_data.flatten())
            overall_correlation = correlation_matrix[0, 1] if not np.isnan(correlation_matrix[0, 1]) else 0.0
            
            # è®¡ç®—RÂ²åˆ†æ•°ï¼ˆå†³å®šç³»æ•°ï¼‰
            ss_res = np.sum((original_data - reconstructed_data) ** 2)
            ss_tot = np.sum((original_data - np.mean(original_data)) ** 2)
            r2_score = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0
            
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ç»Ÿè®¡æŒ‡æ ‡
            num_nodes = original_data.shape[1]
            node_statistics = []
            
            for node_idx in range(num_nodes):
                orig_node = original_data[:, node_idx]
                recon_node = reconstructed_data[:, node_idx]
                
                node_mse = np.mean((orig_node - recon_node) ** 2)
                node_mae = np.mean(np.abs(orig_node - recon_node))
                node_rmse = np.sqrt(node_mse)
                
                # èŠ‚ç‚¹ç›¸å…³ç³»æ•°
                if np.std(orig_node) > 0 and np.std(recon_node) > 0:
                    node_corr = np.corrcoef(orig_node, recon_node)[0, 1]
                    if np.isnan(node_corr):
                        node_corr = 0.0
                else:
                    node_corr = 0.0
                
                # èŠ‚ç‚¹RÂ²åˆ†æ•°
                node_ss_res = np.sum((orig_node - recon_node) ** 2)
                node_ss_tot = np.sum((orig_node - np.mean(orig_node)) ** 2)
                node_r2 = 1 - (node_ss_res / node_ss_tot) if node_ss_tot != 0 else 0.0
                
                # èŠ‚ç‚¹æ•°æ®èŒƒå›´
                orig_range = [float(np.min(orig_node)), float(np.max(orig_node))]
                recon_range = [float(np.min(recon_node)), float(np.max(recon_node))]
                
                node_statistics.append({
                    'node_id': node_idx + 1,
                    'mse': float(node_mse),
                    'mae': float(node_mae),
                    'rmse': float(node_rmse),
                    'correlation': float(node_corr),
                    'r2_score': float(node_r2),
                    'original_range': orig_range,
                    'reconstructed_range': recon_range,
                    'original_mean': float(np.mean(orig_node)),
                    'reconstructed_mean': float(np.mean(recon_node)),
                    'original_std': float(np.std(orig_node)),
                    'reconstructed_std': float(np.std(recon_node))
                })
            
            # å‡†å¤‡å®Œæ•´çš„ç»Ÿè®¡æŠ¥å‘Š
            statistics = {
                'dataset_info': {
                    'original_data_shape': original_data.shape,
                    'reconstructed_data_shape': reconstructed_data.shape,
                    'num_timesteps': int(original_data.shape[0]),
                    'num_nodes': int(original_data.shape[1]),
                    'data_type': 'CSVæ—¶é—´åºåˆ—æ•°æ®ï¼ˆè¡Œä¸ºæ ·æœ¬æ•°ï¼Œåˆ—ä¸ºèŠ‚ç‚¹ç§ç±»ï¼‰'
                },
                'overall_metrics': {
                    'mse': float(mse),
                    'mae': float(mae),
                    'rmse': float(rmse),
                    'correlation': float(overall_correlation),
                    'r2_score': float(r2_score)
                },
                'data_statistics': {
                    'original_data_range': [float(np.min(original_data)), float(np.max(original_data))],
                    'reconstructed_data_range': [float(np.min(reconstructed_data)), float(np.max(reconstructed_data))],
                    'original_data_mean': float(np.mean(original_data)),
                    'reconstructed_data_mean': float(np.mean(reconstructed_data)),
                    'original_data_std': float(np.std(original_data)),
                    'reconstructed_data_std': float(np.std(reconstructed_data))
                },
                'node_wise_metrics': node_statistics,
                'best_reconstructed_nodes': {
                    'highest_correlation': sorted(node_statistics, key=lambda x: x['correlation'], reverse=True)[:3],
                    'lowest_mse': sorted(node_statistics, key=lambda x: x['mse'])[:3],
                    'highest_r2': sorted(node_statistics, key=lambda x: x['r2_score'], reverse=True)[:3]
                },
                'visualization_info': {
                    'max_timesteps_displayed': int(min(200, original_data.shape[0])),
                    'total_available_timesteps': int(original_data.shape[0]),
                    'selected_for_visualization': f"å‰{min(200, original_data.shape[0])}ä¸ªæ—¶é—´æ­¥"
                }
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            import json
            stats_path = os.path.join(self.results_dir, "reconstruction_statistics.json")
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, indent=2, ensure_ascii=False)
            
            print(f"   âœ… é‡æ„ç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_path}")
            
            # ä¿å­˜åŸå§‹å’Œé‡æ„æ•°æ®
            np.save(os.path.join(self.results_dir, "original_data.npy"), original_data)
            np.save(os.path.join(self.results_dir, "reconstructed_data.npy"), reconstructed_data)
            print(f"   ğŸ’¾ åŸå§‹å’Œé‡æ„æ•°æ®å·²ä¿å­˜ä¸º.npyæ–‡ä»¶")
            
            # æ‰“å°ç®€è¦ç»Ÿè®¡
            print(f"   ğŸ“Š é‡æ„æ€§èƒ½æ¦‚è§ˆ:")
            print(f"      æ•´ä½“MSE: {mse:.6f}")
            print(f"      æ•´ä½“ç›¸å…³ç³»æ•°: {overall_correlation:.3f}")
            print(f"      æ•´ä½“RÂ²åˆ†æ•°: {r2_score:.3f}")
            print(f"      æœ€ä½³é‡æ„èŠ‚ç‚¹(ç›¸å…³ç³»æ•°): èŠ‚ç‚¹{statistics['best_reconstructed_nodes']['highest_correlation'][0]['node_id']}")
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜é‡æ„ç»Ÿè®¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def run_all_visualizations(self, train_losses=None, val_losses=None, loss_components=None,
                              predicted_matrix=None, true_matrix=None,
                              model=None, test_data=None, raw_data=None):
        """è¿è¡Œæ‰€æœ‰å¯è§†åŒ– - ç»Ÿä¸€å…¥å£"""
        print(f"\n{'='*60}")
        print(f"              è¿è¡ŒPFNOå¯è§†åŒ–å¥—ä»¶")
        print(f"{'='*60}")
        
        success_count = 0
        total_tasks = 0
        
        # 1. è®­ç»ƒæ›²çº¿å¯è§†åŒ–
        if train_losses is not None and val_losses is not None:
            total_tasks += 1
            print(f"\nğŸ¯ ä»»åŠ¡ 1/3: è®­ç»ƒæ›²çº¿å¯è§†åŒ–")
            if self.plot_training_curves_enhanced(train_losses, val_losses, loss_components or {}):
                success_count += 1
        
        # 2. çŸ©é˜µå¯¹æ¯”å¯è§†åŒ–
        if predicted_matrix is not None:
            total_tasks += 1
            print(f"\nğŸ¯ ä»»åŠ¡ 2/3: é‚»æ¥çŸ©é˜µå¯¹æ¯”å¯è§†åŒ–")
            if self.plot_matrix_comparison_enhanced(predicted_matrix, true_matrix):
                success_count += 1
        
        # 3. æ—¶é—´åºåˆ—é‡æ„å¯è§†åŒ–
        if model is not None and test_data is not None:
            total_tasks += 1
            print(f"\nğŸ¯ ä»»åŠ¡ 3/3: æ—¶é—´åºåˆ—é‡æ„å¯è§†åŒ–")
            if self.generate_time_series_visualizations(model, test_data, raw_data):
                success_count += 1
        
        print(f"\n{'='*60}")
        print(f"    å¯è§†åŒ–å¥—ä»¶å®Œæˆ: {success_count}/{total_tasks} ä¸ªä»»åŠ¡æˆåŠŸ")
        print(f"{'='*60}")
        
        return success_count, total_tasks


# æ›´æ–° __all__ åˆ—è¡¨
__all__ = [
    'plot_adjacency_matrix',
    'plot_training_curves', 
    'plot_evaluation_metrics',
    'plot_comparison_results',
    'plot_threshold_analysis',
    'create_experiment_dashboard',
    'save_plots',
    'plot_probability_matrix_comparison',
    'plot_time_series_reconstruction',
    'plot_comprehensive_results',
    'plot_enhanced_time_series_reconstruction',
    'PFNOVisualizationSuite'  # æ–°å¢ç»Ÿä¸€å¯è§†åŒ–å¥—ä»¶
] 